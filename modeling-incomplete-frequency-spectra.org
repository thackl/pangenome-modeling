** Model Incompleteness as a Binomial Process on Frequency Spectrum

Each true frequency ("bar") can be modelled as its own binomial distribution (or
better something similar to a binomial distribution, but with smaller variance
in case of different completeness values for different genomes)

Use something like an MLE/EM to fit one binomial distribution foreach 1
... G. The binomial for G should be a good basis for core genome size
estimates...

Once we've established this, we can
- a) apply IMG (maybe to a range of likely estimations, given pangenome and
  supergenome size estimate ranges
- b) refine per gene probabilities of being core given individual genome
  completeness, using a conditional probability approach

*** model frequencies of incomplete data with binomial

#+BEGIN_SRC R
library(tidyverse)

# 100 genomes
# 1000 core genes

t0 <- data_frame(
    genomes = c(80:99, 100),
    count = c(rnorm(length(genomes)-1,100, 50),1000)
)

sum(t0$count)

ggplot(t0) + aes(genomes, count) + geom_col()

p <- 0.8


# this doesn't make sense, cause it samples clustered within COG, not within Genome
clustered_binomial <- function(size, min, max){
    y <- sample(min:max,1)
    sum(y*replicate(size/y, rbinom(1 ,1, runif(1, 0.6,1))))
}


t1 <- bind_rows(.id="Completeness",
                `0_p100` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=rbinom(.$count,.$genomes,1))),
                `1_p099.9` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=rbinom(.$count,.$genomes,0.999))),
                `2_p080` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=rbinom(.$count,.$genomes,0.80))),
                `3_p060-100` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=replicate(.$count, sum(replicate(.$genomes, rbinom(1 ,1, runif(1, 0.6,1))))))))


t1 %>% group_by(Completeness) %>% summarize(sum(seen))

b1 <- data_frame(
    x=1:100,
    y=dbinom(x,100,.80) * 2000)

ggplot(t1) +
    aes(seen, fill=as.character(genomes)) +
    geom_bar() +
    geom_line(aes(x,y, fill=NA), b1) +
    xlim(0,101) +
    facet_wrap(~Completeness, ncol=1)

t1 %>% filter(genomes==100)

ggsave("freq-spectrum-core-binomial.png")

rbinom(1, 100, 0.8)
library(mixtools)



binom
#+END_SRC

*** fit data by Maximum Likelihood Estimation (MLE)


Solve by MLE

Binomial Likelihood & Log-Likelihood
- https://rpubs.com/felixmay/MLE
- https://sites.warnercnr.colostate.edu/gwhite/wp-content/uploads/sites/73/2017/04/BinomialLikelihood.pdf
- http://apps.usd.edu/coglab/psyc792/pdf/BinomialLikelihoodFunction.pdf
- http://shiny.stat.calpoly.edu/MLE_Binomial/
- https://somerealnumbers.wordpress.com/2015/07/06/maximum-likelihood-fitting-with-r/


Fitting a mixture model
- http://tinyheero.github.io/2016/01/03/gmm-em.html



#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)

#' @param n total number of trials
#' @param spw parameters of the mixture components as data.frame with columns
#' 'size','prob' and 'weight'
rbinoX <- function(n, sizes, probs, weights=1){
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
        mutate(weight=weight*n/sum(weight)) # this is the worst randomizer :)
    apply(spw, 1, function(r){
        rbinom(r[3], r[1], r[2])}) %>% unlist %>% as.vector
}


n <- 100
p <- 0.8
weights <- c(1,5)
weights_3 <- c(10, rep(1,19), 10)

f0 <- bind_rows(
    .id="fun",
    binom=data_frame(x=rbinom(n, 100, p)),
    binoX_1=data_frame(x=rbinoX(n, 20:50, p)),
    binoX_2=data_frame(x=rbinoX(n, c(50,80), p, weights)),
    binoX_3=data_frame(x=rbinoX(n, c(50:70), p, weights_3))
)

f1 <- f0 %>% group_by(fun, x) %>%
    summarize(y=n())

f1_gg <- ggplot(f1) + aes(x, y, color=fun, fill=fun) +
    geom_pointrange(aes(ymin=0, ymax=y), shape=22) +
    facet_wrap(~fun, ncol=1)
f1_gg

# The PMF of a mixture (for a finite set of discrete distributions) is the
# weighted sum of the component distributions f(x) = sum(w_i * p_i(x)) for
# i:=1..n
# https://en.wikipedia.org/wiki/Mixture_distribution
dbinoX_superslow <- function(x, sizes, probs, weights=1, log=FALSE){
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
            mutate(weight=weight/sum(weight)) # relative weight
    y <- sapply(x, function(k){ # for each observed value
        sum(apply(spw, 1, function(l){ # sum over each params (sizes+probs) * weight
            dbinom(k, l[1], l[2]) * l[3]
        }))
    })
    if(log==TRUE) log(y) else y
}

dbinoX_slow <- function(x, sizes, probs, weights=1, log=FALSE){
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
            mutate(weight=weight/sum(weight)) # relative weight
    y <- apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}) %>%
        rowSums
    if(log==TRUE) log(y) else y
}

dbinoX <- function(x, sizes, probs, weights=1, log=FALSE){
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}


## plot some toy data
x <- 1:n
p <- 0.8

# test functions
dbinoX_superslow(x, c(50,80), p, weights)
dbinoX_slow(x, c(50,80), p, weights)
dbinoX(x, c(50,80), p, weights)

f2 <- bind_rows(
    .id="fun",
    binom=data_frame(
        x=x,
        y=dbinom(x, 100, p) * n),
    binoX_1=data_frame(
        x=x,
        y=dbinoX(x, 20:50, p) * n),
    binoX_2=data_frame(
        x=x,
        y=dbinoX(x, c(50,80), p, weights)* n),
    binoX_3=data_frame(
        x=x,
        y=dbinoX(x, c(50:70), p, weights_3)* n)

)
f2


f2_gg <- f1_gg + geom_point(data=f2) + geom_line(data=f2, linetype=3)
f2_gg

# For the binomial distr. B(n,p), the likelihood L(theta|x) for a given
# parameter (theta) given the observed data (x), is given by the PMF of the
# binomial distr.: L(theta|x) = p_theta(x) = P_theta(X=x)
# https://en.wikipedia.org/wiki/Likelihood_function
#
# And from the mixture rule above it follows for the Likelihood of a binomial
# mixture:


LL_binoX <- function(...){
    -sum(dbinoX(..., log=TRUE))
}


# optim ######################################################################
# http://www.magesblog.com/2013/03/how-to-use-optim-in-r.html
# optimize weights
x <- f0 %>% filter(fun=="binoX_2") %>% pull(x)
sizes <- c(50,80)
probs <- p
weights_initial <- rep(1, length(sizes))
MLE <- optim(weights_initial, LL_binoX, x=x, sizes=sizes, probs=probs,
             method="L-BFGS-B", lower=0, upper=1, control=list(trace=TRUE))
MLE
(weights_mle <- MLE$par / sum(MLE$par))


x_3 <- f0 %>% filter(fun=="binoX_3") %>% pull(x)
sizes_3 <- c(50:70)
probs <- p
weights_initial_3 <- rep(1, length(sizes_3))

MLE <- optim(weights_initial_3, LL_binoX, x=x_3, sizes=sizes_3, probs=probs,
             method="L-BFGS-B", lower=0, upper=1, control=list(trace=TRUE))
MLE
(weights_mle_3 <- MLE$par / sum(MLE$par))


LL_binoX(x_3, sizes_3, probs, weights_initial_3)
LL_binoX(x_3, sizes_3, probs, weights_3)
LL_binoX(x_3, sizes_3, probs, weights_mle_3)


## ?? Might be worth looking into
MLE3 <- constrOptim(weights_initial, LL_binoX, x=x, sizes=sizes, probs=probs,
             method="L-BFGS-B", ui)


# mle2 ######################################################################
library(bbmle)

ll <- function(w1, w2){
    -sum(dbinoX(x, sizes, probs, weights=c(w1, w2), log=TRUE))
}

MLE2 <- mle2(
    ll, method="L-BFGS-B",
    lower=c(w1=0, w2=0),
    upper=c(w1=1, w2=1),
    start=list(w1=0.5,w2=0.5),
    control=list(trace=TRUE))
MLE2
(weights_mle2 <- coef(MLE2) / sum(coef(MLE2)))

MLE2 <- mle2(
    ll, method="L-BFGS-B",
    lower=c(w1=0, w2=0),
    upper=c(w1=1, w2=1),
    start=list(w1=0.5,w2=0.5),
    control=list(trace=TRUE))
MLE2
(weights_mle2 <- coef(MLE2) / sum(coef(MLE2)))


MLE
MLE2

# plot ######################################################################
x <- 1:n
f3 <- bind_rows(
    .id="fun",
#    binoX_1=data_frame(
#        x=x,
#        y=dbinoX(x, 20:50, p, weights_mle) * n),
    binoX_2=data_frame(
        x=x,
        y=dbinoX(x, c(50,80), p, weights_mle2)* n),
    binoX_3=data_frame(
        x=x,
        y=dbinoX(x, sizes_3, p, weights_mle_3)* n)
)

f2_gg + geom_point(data=f3, shape=5) + geom_line(data=f3, linetype=5, color="blue")


# MLE standard error / cofidence intervals
# https://stats.stackexchange.com/questions/27033/in-r-given-an-output-from-optim-with-a-hessian-matrix-how-to-calculate-paramet



################################################################################
## ************************************************************************** ##
################################################################################
## plot pangenome data
p <- 0.8
weights <- c(1000, rnbinom(98,10,mu=100), 1000) # unnormalized weights (sum != 1)
n <- sum(weights) # total genes
sizes <- 1:100

p0 <- bind_rows(
    .id="fun",
    p100=data_frame(x=rbinoX(n, sizes, 1, weights)),
    p80=data_frame(x=rbinoX(n, sizes, p, weights))
)
p1 <- p0 %>% group_by(fun, x) %>%
    summarize(y=n())

p1_gg <- ggplot(p1) + aes(x, y, color=fun, fill=fun) +
    geom_pointrange(aes(ymin=0, ymax=y), shape=22) +
    facet_wrap(~fun, ncol=1)
p1_gg

x <- p0 %>% filter(fun=="p80") %>% pull(x)
weights_initial <- rep(1, length(sizes))


## optim
MLE <- optim(weights_initial, LL_binoX, x=x, sizes=sizes, probs=p,
             method="L-BFGS-B", lower=0, upper=1)
MLE

MLE <- optim(rep(1, 11), LL_binoX, x=x, sizes=c(90:100), probs=p,
             method="L-BFGS-B", lower=0.01)
MLE



LL_binoX(x=x, sizes=sizes, probs=p, weights_initial)


(weights_mle <- MLE$par / sum(MLE$par))

x <- 1:n
f3 <- bind_rows(
    .id="fun",
#    binoX_1=data_frame(
#        x=x,
#        y=dbinoX(x, 20:50, p, weights_mle) * n),
    binoX_2=data_frame(
        x=x,
        y=dbinoX(x, c(50,80), p, weights_mle)* n)
)

f2_gg + geom_point(data=f3, shape=5) + geom_line(data=f3, linetype=5, color="blue")



################################################################################


rLL_binoX <- function(x, sizes, probs, weights, lambda=1e2, minimum=NULL){
    # only weights > minimum
    if(!is.null(minimum) & any(weights<minimum)) return(Inf)

    # regularized negative log likelihood
    -sum(dbinoX(x, sizes, probs, weights, log=TRUE)) +
        # regularize weights to sum to 1
        lambda * (sum(abs(weights))-1)
}

sum(weights_initial_3)

w4 <- weights_initial_3 / sum(weights_initial_3)

LL_binoX(x_3, sizes_3, probs, weights_initial_3)
rLL_binoX(x_3, sizes_3, probs, weights=weights_initial_3)
rLL_binoX(x_3, sizes_3, probs, weights=weights_initial_3, lambda=10)
rLL_binoX(x_3, sizes_3, probs, weights=w4, lambda=10)
rLL_binoX(x_3, sizes_3, probs, -w4, 10)


freq_sim <- function(d, n=1){
    d$sim <- list()
    for(i in 1:n){
        d$sim[[i]] <- rbinoX(sum(d$true$freq), d$true$size, d$p, d$true$freq) %>%
            table %>% as_data_frame %>%
            select(size=1, freq=2) %>%
            mutate(size=as.numeric(size))
    }
    d
}

freq_fit <- function(d, regularized=FALSE){
    for(i in 1:length(d$sim)){
        # fit mix with same number of components as in truth
        weights_guess <- c(rep(1, length(d$true$size)-2), 10,100) # mostly core
        weights_guess <- weights_guess/sum(weights_guess)
        x <- rep(d$sim[[i]]$size, d$sim[[i]]$freq)
        sizes <- d$true$size
        n <- sum(d$true$freq) # TODO: n when truth unknown?

        if(regularized){
            MLE <- optim(
                weights_guess, rLL_binoX,
                method="Nelder-Mead", control=list(trace=TRUE, maxit=500),
                x=x, sizes=sizes, probs=d$p, minimum=0) # data
        }else{
            MLE <- optim(
                weights_guess, rLL_binoX,
                method="L-BFGS-B", lower=0, control=list(trace=TRUE, maxit=500),
                x=x, sizes=sizes, probs=d$p) # data
        }

        weights_mle <- MLE$par / sum(MLE$par) # normalized weights
        print(weights_mle)

        d$fit[[i]] <- data_frame(
            size=d$range,
            freq=dbinoX(d$range, sizes, p, weights_mle) * n)
        d$est[[i]] <- data_frame(
            size=sizes,
            freq=weights_mle * n)
    }
    d
}

#    freq_true = c(1000, rnbinom(98,10,mu=100), 1000) # =~ unnormalized weights

d <- list(
    p=0.8,
    true = data_frame(
        size = c(981:1000),
        freq = c(rep(100, 18),200,1000)))
d$range <- 1:max(d$true$size)

d <- freq_sim(d, 3)

d <- freq_fit(d, regularized=TRUE)

dev.new()
d1 <- bind_rows(
    .id="fit",
    true=d$true %>% mutate(i="0"),
    est=bind_rows(.id="i", d$est),
    sim=bind_rows(.id="i", d$sim),
    fit=bind_rows(.id="i", d$fit)
)
ggplot(d1) + aes(x=size, y=freq, fill=fit, color=fit) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept=1000) +
    xlim(700,NA) +
#    geom_density(stat="identity", alpha=.2) +
    facet_wrap(~i)


# plot
# - true spectrum
# - observed spectrum

d
gg <- ggplot(dx) + aes(x) + geom_bar(aes(group=size, fill=as.factor(size))) +
    scale_fill_manual(values=rep(brewer.pal(10,"Spectral"), times=10)) +
    facet_wrap(~par)
gg
#+END_SRC

*** regularized MLE with confidence intervals

#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)
library(bbmle)

#' @param n total number of trials
#' @param spw parameters of the mixture components as data.frame with columns
#' 'size','prob' and 'weight'
rbinoX <- function(n, sizes, probs, weights=NULL){
    if(is.null(weights)) weights <- 1
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
        mutate(weight=weight*n/sum(weight)) # this is the worst randomizer :)
    apply(spw, 1, function(r){
        rbinom(r[3], r[1], r[2])}) %>% unlist %>% as.vector
}

dbinoX <- function(x, sizes, probs, weights=NULL, log=FALSE){
    if(is.null(weights)) weights <- 1
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}

LL_binoX <- function(x, sizes, probs, weights, lambda=NULL, lower_bound=NULL){
    # penalize weights < lower_bound bound
    if(!is.null(lower_bound) && any(weights<lower_bound)) return(1e10)
    # negative log likelihood
    ll <- -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
    # regularize weights to sum to 1
    if(!is.null(lambda)) ll <- ll + lambda * (sum(abs(weights))-1)
    ll
}

data_frame(x = rbeta(10000, 1,1)) %>% ggplot() + aes(x) + geom_density()

d <- data_frame(x=1:100, y=dbeta(1:100/100,30,1))
ggplot(d) + aes(x,y) + geom_point()


LL_beta_binoX <- function(x, sizes, probs, alpha, beta, lambda=NULL){
    # get weights from dbeta
    m <- length(sizes)
    weights <- dbeta(1:m/m,alpha,beta)
    # negative log likelihood
    ll <- -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
    # regularize weights to sum to 1
    if(!is.null(lambda)) ll <- ll + lambda * (sum(abs(weights))-1)
    ll
}


freq_sim <- function(d, n=1){
    d$sim <- list()
    for(i in 1:n){
        d$sim[[i]] <- rbinoX(sum(d$true$freq), d$true$size, d$p, d$true$freq) %>%
            table %>% as_data_frame %>%
            select(size=1, freq=2) %>%
            mutate(size=as.numeric(size))}
    d
}

freq_fit <- function(d, wrapper="optim", method="Nelder-Mead", lambda=NULL, lower_bound=NULL, control){
    d$mle <- list()
    d$fit <- list()
    d$est <- list()
    for(i in 1:length(d$sim)){
        # fit mix with same number of components as in truth
        weights_guess <- c(rep(1, length(d$true$size)-2), 10,100) # mostly core
        weights_guess <- weights_guess/sum(weights_guess)
        x <- rep(d$sim[[i]]$size, d$sim[[i]]$freq)
        sizes <- d$true$size
        n <- sum(d$true$freq) # TODO: n when truth unknown?

        if(is.null(wrapper) || wrapper=="optim"){
            MLE <- optim(
                weights_guess, LL_binoX,
                method=method, control=control,
                # ll params
                lambda=lambda, lower_bound=lower_bound,
                # ll data
                x=x, sizes=sizes, probs=d$p)

            weights_mle <- MLE$par / sum(MLE$par) # normalized weights
        }else if(wrapper=="mle2"){
            # make optim-style params compatible with mle2 interface
            weights_guess_names <- paste0("w", 1:length(weights_guess))
            names(weights_guess) <- weights_guess_names
            parnames(LL_binoX) <- weights_guess_names

            MLE <- mle2(
                LL_binoX, method=method,
                start=weights_guess,
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, lambda=lambda, lower_bound=lower_bound))

            weights_mle <- coef(MLE) / sum(coef(MLE)) # normalized weights
        }else if(wrapper=="mle2-beta"){
            MLE <- mle2(
                LL_beta_binoX, method=method,
                start=list(alpha=30, beta=1),
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, lambda=lambda, lower_bound=lower_bound))

            m <- length(sizes)
            ab <- coef(MLE) # estimated alpha and beta from BETA
            weights_mle <- dbeta(1:m/m,ab[1],ab[2])
            weights_mle <- weights_mle/sum(weights_mle) # normalize weights
        }

        d$mle[[i]] <- MLE
        d$fit[[i]] <- data_frame(
            size=d$range,
            freq=dbinoX(d$range, sizes, p, weights_mle) * n)
        d$est[[i]] <- data_frame(
            size=sizes,
            freq=weights_mle * n)
    }
    d
}

#    freq_true = c(1000, rnbinom(98,10,mu=100), 1000) # =~ unnormalized weights

d0 <- list(
    p=0.8,
    true = data_frame(
        size = c(981:1000),
        freq = c(rep(100, 18),200,1000)))
d0$range <- 1:max(d0$true$size)

d1 <- freq_sim(d0, 3)

#d2 <- freq_fit(d1, "mle2", method="Nelder-Mead", lambda=100, lower_bound=0, control=#list(trace=TRUE, maxit=1e4))

d2 <- freq_fit(d1, "mle2-beta", method="Nelder-Mead", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e4))

#d2 <- freq_fit(d1, "optim", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e2))

d3 <- bind_rows(
    .id="fit",
    true=d2$true %>% mutate(i="0"),
    est=bind_rows(.id="i", d2$est),
    sim=bind_rows(.id="i", d2$sim),
    fit=bind_rows(.id="i", d2$fit)
)
ggplot(d3) + aes(x=size, y=freq, fill=fit, color=fit) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept=1000) +
    xlim(700,NA) +
#    geom_density(stat="identity", alpha=.2) +
    facet_wrap(~i)

p1 <- profile(d2$mle[[1]], skip.hessian=TRUE)
p2 <- profile(p1, std.err=2.404e+04)
confint(p2)
coef(p1)

plot(p2, absVal=FALSE)

ci <- confint(d2$mle[[1]])

coef(d2$mle[[1]])
coef(ci)

# plot
# - true spectrum
# - observed spectrum

d
gg <- ggplot(dx) + aes(x) + geom_bar(aes(group=size, fill=as.factor(size))) +
    scale_fill_manual(values=rep(brewer.pal(10,"Spectral"), times=10)) +
    facet_wrap(~par)
gg

# my hessian can't be inverted, so profiling and confidence intervals aren't available..










x <- 0:10
y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8)
d <- data.frame(x,y)
## in general it is best practice to use the data argument,
##  but variables can also be drawn from the global environment
LL <- function(ymax=15, xhalf=6)
-sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
## uses default parameters of LL
(fit <- mle2(LL))
fit1F <- mle2(LL, fixed=list(xhalf=6))

coef(fit1F)
coef(fit1F,exclude.fixed=TRUE)
(fit0 <- mle2(y~dpois(lambda=ymean),start=list(ymean=mean(y)),data=d))
anova(fit0,fit)
summary(fit)
logLik(fit)
vcov(fit)
p1 <- profile(fit)
plot(p1, absVal=FALSE)
confint(fit)

## use bounded optimization
## the lower bounds are really > 0, but we use >=0 to stress-test
## profiling; note lower must be named
(fit1 <- mle2(LL, method="L-BFGS-B", lower=c(ymax=0, xhalf=0)))
p1 <- profile(fit1)
plot(p1, absVal=FALSE)
## a better parameterization:
LL2 <- function(lymax=log(15), lxhalf=log(6))
-sum(stats::dpois(y, lambda=exp(lymax)/(1+x/exp(lxhalf)), log=TRUE))
(fit2 <- mle2(LL2))
plot(profile(fit2), absVal=FALSE)
exp(confint(fit2))
vcov(fit2)
cov2cor(vcov(fit2))
#+END_SRC

*** MLE on normalized weights
Gabe's idea: change of variable:

wi => xi
xi = wi/w1
x1 = w1/w1 = 1 = const (edited)
Dann variieren alle xi zwischen [0,inf)

Gff Dann noch weiter
yi = log(xi)
y1 = 0 = const
Dann variieren alle y zwischen (-inf,inf)
Ausser y1=0
Dann musst du nur bei der likelihood calculation jeweils die y in w zurückrechnen
wi = exp(yi) * w1
Wobei w1 = 1 - sum(w2...wn)
Das ist ein lineares system
Kannst du mit 'solve' lösen

(Kann sein, dass da noch der Wurm drin ist - hab's ohne solve gemacht...)
=> *Konvergiert schneller (Nelder-Mead < 1000), aber Ergebnisse sind nicht besser*

#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)
library(bbmle)

#' @param n total number of trials
#' @param spw parameters of the mixture components as data.frame with columns
#' 'size','prob' and 'weight'
rbinoX <- function(n, sizes, probs, weights=NULL){
    if(is.null(weights)) weights <- 1
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
        mutate(weight=weight*n/sum(weight)) # this is the worst randomizer :)
    apply(spw, 1, function(r){
        rbinom(r[3], r[1], r[2])}) %>% unlist %>% as.vector
}

dbinoX <- function(x, sizes, probs, weights=NULL, log=FALSE){
    if(is.null(weights)) weights <- 1
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}

LL_binoX <- function(x, sizes, probs, weights, lambda=NULL, lower_bound=NULL){
    # penalize weights < lower_bound bound
    if(!is.null(lower_bound) && any(weights<lower_bound)) return(1e10)
    # negative log likelihood
    ll <- -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
    # regularize weights to sum to 1
    if(!is.null(lambda)) ll <- ll + lambda * (sum(abs(weights))-1)
    ll
}

w2x <- function(weights) log(weights[2:length(weights)]/weights[1])
x2w <- function(w1, xeights) c(w1, exp(xeights) *w1)

LL_x_binoX <- function(x, sizes, probs, w1, xeights, lower_bound=NULL){
    weights <- x2w(w1, xeights)
    # negative log likelihood
    -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
}

freq_sim <- function(d, n=1){
    d$sim <- list()
    for(i in 1:n){
        d$sim[[i]] <- rbinoX(sum(d$true$freq), d$true$size, d$p, d$true$freq) %>%
            table %>% as_data_frame %>%
            select(size=1, freq=2) %>%
            mutate(size=as.numeric(size))}
    d
}

freq_fit <- function(d, wrapper="optim", method="Nelder-Mead", lambda=NULL, lower_bound=NULL, control){
    d$mle <- list()
    d$fit <- list()
    d$est <- list()
    for(i in 1:length(d$sim)){
        # fit mix with same number of components as in truth
        weights_guess <- c(rep(1, length(d$true$size)-2), 10,100) # mostly core
        weights_guess <- weights_guess/sum(weights_guess)
        x <- rep(d$sim[[i]]$size, d$sim[[i]]$freq)
        sizes <- d$true$size
        n <- sum(d$true$freq) # TODO: n when truth unknown?

        if(is.null(wrapper) || wrapper=="optim"){
            MLE <- optim(
                weights_guess, LL_binoX,
                method=method, control=control,
                # ll params
                lambda=lambda, lower_bound=lower_bound,
                # ll data
                x=x, sizes=sizes, probs=d$p)

            weights_mle <- MLE$par / sum(MLE$par) # normalized weights
        }else if(wrapper=="mle2"){
            # make optim-style params compatible with mle2 interface
            weights_guess_names <- paste0("w", 1:length(weights_guess))
            names(weights_guess) <- weights_guess_names
            parnames(LL_binoX) <- weights_guess_names

            MLE <- mle2(
                LL_binoX, method=method,
                start=weights_guess,
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, lambda=lambda, lower_bound=lower_bound))

            weights_mle <- coef(MLE) / sum(coef(MLE)) # normalized weights
        }else if(wrapper=="mle2-x"){
            # make optim-style params compatible with mle2 interface
            w1 <- weights_guess[1]
            xeights_guess <- w2x(weights_guess)
            xeights_guess_names <- paste0("x", 1:length(xeights_guess))
            names(xeights_guess) <- xeights_guess_names
            parnames(LL_x_binoX) <- xeights_guess_names

            MLE <- mle2(
                LL_x_binoX, method=method,
                start=xeights_guess,
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, w1=w1, lower_bound=lower_bound))

            weights_mle <- x2w(w1, coef(MLE))
            weights_mle <- weights_mle/sum(weights_mle) # normalized weights
        }

        d$mle[[i]] <- MLE
        d$fit[[i]] <- data_frame(
            size=d$range,
            freq=dbinoX(d$range, sizes, d$p, weights_mle) * n)
        d$est[[i]] <- data_frame(
            size=sizes,
            freq=weights_mle * n)
    }
    d
}

#    freq_true = c(1000, rnbinom(98,10,mu=100), 1000) # =~ unnormalized weights

d0 <- list(
    p=0.8,
    true = data_frame(
        size = c(981:1000),
        freq = c(rep(100, 18),200,1000)))
d0$range <- 1:max(d0$true$size)

d1 <- freq_sim(d0, 3)

#d2 <- freq_fit(d1, "mle2", method="Nelder-Mead", lambda=100, lower_bound=0, control=#list(trace=TRUE, maxit=1e4))

d2 <- freq_fit(d1, "mle2-x", method="Nelder-Mead", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e4))

#d2 <- freq_fit(d1, "optim", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e2))

d3 <- bind_rows(
    .id="fit",
    true=d2$true %>% mutate(i="0"),
    est=bind_rows(.id="i", d2$est),
    sim=bind_rows(.id="i", d2$sim),
    fit=bind_rows(.id="i", d2$fit)
)
ggplot(d3) + aes(x=size, y=freq, fill=fit, color=fit) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept=1000) +
    xlim(700,NA) +
#    geom_density(stat="identity", alpha=.2) +
    facet_wrap(~i)

d2$mle[[1]]

p1 <- profile(d2$mle[[1]])
p2 <- profile(p1, std.err=2.403e+04)
confint(p1)
coef(p1)

plot(p2, absVal=FALSE)

ci <- confint(d2$mle[[1]])

coef(d2$mle[[1]])
coef(ci)
#+END_SRC

** Bayesian Inference
*** Background
Really useful intro video on how to apply bayesian data analysis/inference

- Part 1: https://www.youtube.com/watch?v=3OJEae7Qb_o
  - exercise: https://goo.gl/cxfnYK (or http://www.sumsar.net/files/posts/2017-bayesian-tutorial-exercises/modeling_exercise1.html)
- Part 2: https://www.youtube.com/watch?v=mAUwjSo5TJE
- Part 3: https://www.youtube.com/watch?v=Ie-6H_r7I5A
  - Stan


Books:
- "Doing Bayesian Data Analysis", John K. Kruschke
- "Statistical Rethinking", Richard McElreath

*** Stan/rstan examples
**** "8schools"

#+BEGIN_SRC R
library("rstan") # observe startup messages
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

stan_model_8schools <- "
// saved as 8schools.stan
data {
  int<lower=0> J; // number of schools
  real y[J]; // estimated treatment effects
  real<lower=0> sigma[J]; // s.e. of effect estimates
}
parameters {
  real mu;
  real<lower=0> tau;
  real eta[J];
}
transformed parameters {
  real theta[J];
  for (j in 1:J)
    theta[j] = mu + tau * eta[j];
}
model {
  target += normal_lpdf(eta | 0, 1);
  target += normal_lpdf(y | theta, sigma);
}
"

schools_dat <- list(J = 8,
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))

fit <- stan(model_code=stan_model_8schools, data = schools_dat,
            iter = 1000, chains = 4)

print(fit)
plot(fit)
#+END_SRC

**** Mike Lawrence tutorial
- Video: https://www.youtube.com/watch?v=ev2xpOKxbDQ&index=1&list=PLu77iLvsj_GPoC6tTw01EP1Tcr2I6zEm8

#+BEGIN_SRC R
#one group, known error variance

#clear workspace to ensure a fresh start
rm(list=ls())

#load rstan
library(rstan)

#load ggmcmc
library(ggmcmc)

#set the random seed (so we all generate the same fake data)
set.seed(1)

#generate some fake data
Y <- rnorm(1e4,100,15)

#package the data for stan
data <- list(
	nY = length(Y)
	, Y = Y
)

stan_model <- '
// one group, known error variance
data {
  int nY ; // initialize a variable to indicate the number of elements in Y
  vector[nY] Y ; // initialize a vector to hold the observations
}
parameters { // what we want to infer
  real mu ; // mean of the data-generating population
}
model {
  // priors
  mu ~ normal(100,20) ;

  // generator
  Y ~ normal(mu,15);
}
'

model <- rstan::stan_model(model_code=stan_model)

#evaluate the model
sampling_iterations <- 1e4 #best to use 1e3 or higher
out <- rstan::sampling(
	object = model
	, data = data
	, chains = 1
	# , chains = 4
	# , cores = 4
	, iter = sampling_iterations
	, warmup = sampling_iterations/2
	, refresh = sampling_iterations/10 #show an update @ each %10
	, seed = 1
)

#print a summary table
print(out)
#look at n_eff and RHat to see if we've sampled enough
#    we generally want n_eff>1e3 & Rhat==1
#    if these criteria are not met, run again with more iterations


#extract the posterior groups in a format that ggmcmc likes
samples <- ggmcmc::ggs(out)

#show the histogram of sampled mu values
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_histogram'
)


#look at the traceplot (should look like white noise)
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_traceplot'
)

#look at the full-vs-partial density (should look the same)
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_compare_partial'
)

#look at the auto-correlations
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_autocorrelation'
)
library(shinystan)
shinystan::launch_shinystan(out)
#+END_SRC

**** Hierarchical Logistic Regression from the Stan Manual
Guide, U. ’s, & Manual, R. (n.d.). Stan Modeling Language.
https://paperpile.com/app/p/a53d0165-7672-07dd-a014-9a99a334d1fc
[[chrome-extension://bomfdkbfpdhijjbeoicnfhjbdhncfhig/view.html?mp=ePqCTTS3]] p138

This is not exactly what I need, but it might be useful - it defines a vector of
parameters to be estimated for a series of classes, and some dynamic prior
assignment to these parameters - so quite similar to the weights I am interested
in

**** Finite mixture models in Stan
http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html


#+BEGIN_SRC R
library(dplyr);
library(ggplot2);
library(ggmcmc);

# Number of data points
N <- 400

# Let's make three states
mu <- c(3, 6, 9)
sigma <- c(2, 4, 3)

# with probability
Theta <- c(.5, .2, .3)

# Draw which model each belongs to
z <- sample(1:3, size = N, prob = Theta, replace = T)

# Some white noise
epsilon <- rnorm(N)

# Simulate the data using the fact that y ~ normal(mu, sigma) can be
# expressed as y = mu + sigma*epsilon for epsilon ~ normal(0, 1)
y <- mu[z] + sigma[z]*epsilon

data_frame(y, z = as.factor(z)) %>%
  ggplot(aes(x = y, fill = z)) +
  geom_density(alpha = 0.3)

model_stan_code <- '
data {
  int N;
  vector[N] y;
  int n_groups;
}
parameters {
  vector[n_groups] mu; // use ordered to fix label switching
  vector<lower = 0>[n_groups] sigma;
  simplex[n_groups] Theta;
}
model {
  vector[n_groups] contributions;
  // priors
  mu ~ normal(0, 10);
  sigma ~ cauchy(0, 2);
  Theta ~ dirichlet(rep_vector(2.0, n_groups));


  // likelihood
  for(i in 1:N) {
    for(k in 1:n_groups) {
      contributions[k] = log(Theta[k]) + normal_lpdf(y[i] | mu[k], sigma[k]);
    }
    target += log_sum_exp(contributions);
  }
}'

model_stan_compiled <- stan_model(model_code=model_stan_code)

model <- sampling(
    model_stan_compiled,
    data = list(N= N, y = y, n_groups = 3),
    iter = 600)

S <- ggs(model)
ggs_traceplot(S)
ggs_caterpillar(S)
#+END_SRC

*** Old Faithful bimodal with Dirichelet Mixture (python though)
http://austinrochford.com/posts/2016-02-25-density-estimation-dpm.html

*** Bayesian modeling of a binomial mixture using Stan

#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

library(ggmcmc)


#' @param n total number of observations
#' @param sizes number of trials for each component (single value will be recycled)
#' @param probs probabilities of success for each component (single value will be recycled)
#' @param weights of components, i.e. proportion of observation from total. Values will be normalized to sum to 1. Single values will be recycled.
#' 'size','prob' and 'weight'
#' @param df if TRUE, return data.frame including parameters used to simulate
#' each outcome.
rbinoX <- function(n, sizes, probs, weights=NULL, df=FALSE){
    if(is.null(weights)) weights <- 1
    cns <- sapply(list(sizes, probs, weights), length)
    cn <- max(cns)

    if(any(cns!=cn & cns!=1)) stop("sizes, probs and weights lengths differ. Need to either be the same or 1 (recycling)")

    if(length(sizes)==1) sizes <- rep(sizes, cn)
    if(length(probs)==1) probs <- rep(probs, cn)
    if(length(weights)==1) weights <- rep(weights, cn)

    r <- t(sapply(sample(1:cn, n, replace=TRUE, prob=weights), function(ci){
        c(sizes[ci], probs[ci], weights[ci], rbinom(1, sizes[ci], probs[ci]))}))
    if(df){
        r <- as.data.frame(r)
        colnames(r) <- c("size", "prob","weight","y")
        r
    }else{r[,4]}
}

dbinoX <- function(x, sizes, probs, weights=NULL, log=FALSE){
    if(is.null(weights)) weights <- 1
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}

set.seed(1337)
n <- 1e3
sizes <- c(81:100)
p <- .8
weights <- c(1:20)
weights <- weights/sum(weights)
sizes
weights

d1 <- rbinoX(n, sizes, p, weights, df=T) %>% tbl_df
d1

theme_set(theme_bw())

ggplot(d1) + aes(x=y, fill=as.factor(size)) +
#    geom_area(aes(color=as.factor(size), ),alpha=.1, stat="bin", binwidth=1, position="dodge") +
    geom_bar() +
    xlim(0,100)


#package the data for stan
data <- list(
    # observations
    N = length(d1$y), # n/o observations
    y = d1$y,
    # mixture components
    K = length(sizes), # n/o components
    sizes = sizes,
    probs = rep(p, length(sizes)),
    # priors
    phi = weights/sum(weights), # maximum knowledge
    kappa = 100 # strength
)

model_stan_compiled <- stan_model("binom-mix-repara-dirichlet.stan")

model <- sampling(
    model_stan_compiled,
    data = data,
    iter = 500, chains=4)

model_k1000 <- model
model_k100 <- model

print(model)
S <- ggs(model) %>% filter(grepl("theta", Parameter))
ggs_traceplot(S) + facet_wrap(~Parameter, ncol=5)

ggs_caterpillar(S) + geom_point(
    aes(x=x, y=Parameter),
    data=data_frame(x=weights, Parameter=unique(S$Parameter)),
    color="red", size=6, shape=124)
ggsave("Weight-caterpillar.png")

S2 <- S %>% group_by(Parameter) %>% summarize(
    mean=mean(value),
    q025=quantile(value,.025),
    q975=quantile(value,.975),
    q25=quantile(value,.25),
    q75=quantile(value,.75)) %>% ungroup

ggplot(S2) + aes(x=Parameter, y=mean, group=NA) +
    geom_ribbon(aes(ymin=q025, ymax=q975), alpha=.1) +
    geom_ribbon(aes(ymin=q25, ymax=q75), alpha=.3) +
    geom_line(color="blue", size=1, linetype=3, alpha=.8) +
    geom_point(
        aes(y=x),
        data=data_frame(x=weights, Parameter=unique(S$Parameter)),
        color="red", size=2)
ggsave("Weight-fit.png")


ggs_autocorrelation(S)
ggsave("Weight-autocorrelation.png")
ggs_crosscorrelation(S)
ggsave("Weight-crosscorrelation.png")

# Reparameterization of Dirichlet to formulate priors
# see Reparameterization of Dirichlet Priors (Stan Manual p282)
# https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E8%81%9A%E7%B1%BB/LDA/docs/dirichlet.pdf

require(MCMCpack)
library(ade4)

phi <- c(.5,.1,.1)
phi <- phi/sum(phi)
kappa <- c(10, 10, 10)
(alpha_rp <- kappa * phi)
dr <- MCMCpack::rdirichlet(1000, alpha_rp) %>% tbl_df
dr
triangle.plot(dr, min3=c(0,0,0), max3=c(1,1,1))
#+END_SRC

*** REJC Simulate data from Stan - doesn't work with mixture models that only increment likelyhoods desities
    CLOSED: [2017-10-24 Tue 14:56]

#+BEGIN_SRC R
library(rstan)
library(tidyverse)

sm <- "
parameters {
  real y;
}
model {
  y ~ normal(1,1);
}
"

stan(model_code=sm)



scode <- "
     parameters {
       real y[2];
     }
     model {
       y[1] ~ normal(0, 1);
       y[2] ~ double_exponential(0, 2);
     }
     "
fit1 <- stan(model_code = scode, iter = 10, verbose = FALSE)
print(fit1)
fit2 <- stan(fit = fit1, iter = 10000, verbose = FALSE)

## extract samples as a list of arrays
e2 <- extract(fit2, permuted = TRUE)
## using as.array on the stanfit object to get samples
a2 <- as.array(fit2)
a2 %>% tbl_df
#+END_SRC

*** IMG in Stan
The biggest problem with robustly modeling the true frequency spectrum from
incomplete data via Bayesian Inference is the choice and parameterization of the
priors. Ideally, one would like to estimate the IMG gene gain/loss parameter
directly, and mixing rates of the underlying binomials only indirectly.

IMG:
- gain rate :: mu
- loss rate :: nu
- N :: population size
in scaled coalescent units:
- gain rate :: theta = 2 N mu
- loss rate :: rho = 2 N nu

#+BEGIN_SRC R
library(tidyverse)
theme_set(theme_bw())
library(rstan)
library(ggmcmc)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

#' @param n total number of observations
#' @param sizes number of trials for each component (single value will be recycled)
#' @param probs probabilities of success for each component (single value will be recycled)
#' @param weights of components, i.e. proportion of observation from total. Values will be normalized to sum to 1. Single values will be recycled.
#' 'size','prob' and 'weight'
#' @param df if TRUE, return data.frame including parameters used to simulate
#' each outcome.
rbinoX <- function(n, sizes, probs, weights=NULL, df=FALSE){
    if(is.null(weights)) weights <- 1
    cns <- sapply(list(sizes, probs, weights), length)
    cn <- max(cns)

    if(any(cns!=cn & cns!=1)) stop("sizes, probs and weights lengths differ. Need to either be the same or 1 (recycling)")

    if(length(sizes)==1) sizes <- rep(sizes, cn)
    if(length(probs)==1) probs <- rep(probs, cn)
    if(length(weights)==1) weights <- rep(weights, cn)

    r <- t(sapply(sample(1:cn, n, replace=TRUE, prob=weights), function(ci){
        c(sizes[ci], probs[ci], weights[ci], rbinom(1, sizes[ci], probs[ci]))}))
    if(df){
        r <- as.data.frame(r)
        colnames(r) <- c("size", "prob","weight","y")
        r
    }else{r[,4]}
}


## Collins, R. E., & Higgs, P. G. (2012). Testing the infinitely many genes model for the evolution of the bacterial core genome and pangenome. Molecular Biology and Evolution, 29(11), 3413–3425.
source("../pangenome-infinitely-many-genes/f-pangenome.R")

# rho1, theta1, core, rho2, theta2
rho1 <- .5    # rate of loosing a single gene with time units 2N_e
theta1 <- 200  # average number of genes gained in 2N_e
ng <- 10
f.coalescent(c(rho1, theta1), ng) %>%
    tbl_df %>%
    mutate(n=1:ng) %>%
    gather(set, value, -n) %>%
#    mutate(value=00) %>%
    ggplot() + aes(x=n, y=value, color=set) + geom_line()

fc0 <- f.coalescent.spec(c(rho1, theta1), ng) %>% tbl_df %>% mutate(n=1:ng)
ggplot(fc0) + aes(x=n, y=value) + geom_col()


##---------------------------------------------------

foo <- function(K, rho1, theta1) {

  spec <- (1:K)*0

  (specprod1 <- (K+1 - 1:K)/(K+rho1 - 1:K))

  for (k in 1:K) {
      spec[k] <- (theta1/k)*prod(specprod1[1:k])
  }

  return(spec)
}

fc1 <- foo(ng, rho1, theta1) %>% tbl_df %>% mutate(n=1:ng)
bind_rows(.id="set", ref=fc0, me=fc1) %>%
    ggplot() + aes(x=n, y=value, fill=set) + geom_col(position="dodge")


fc0
## model_stan_compiled <- stan_model("foo.stan")
## model <- sampling(
##    model_stan_compiled,
##     algorithm="Fixed_param",
##     iter = 1, chains=1, data=list(K=ng, rho=rho1, theta=theta1))



model_stan_compiled <- stan_model("img-freqspec-coalescent.stan")
#model_stan_compiled <- readRDS("img-freqspec-coalescent.rds")

m_img_coal_1 <- sampling(
    model_stan_compiled,
    iter = 1000, chains=4,
    data=list(
        N=nrow(fc1), y=fc1$value, K=ng))


model <- m_img_coal_1
print(model)
S <- ggs(model)
ggs_traceplot(S) + facet_wrap(~Parameter, ncol=5)
ggs_caterpillar(S)


## S2 <- S %>% spread(Parameter, value) %>%
##     apply(1, function(r) foo(ng, r[3], r[4]) %>% tbl_df %>% mutate(n=1:ng, ic=paste0(r[1], r[2]))) %>% bind_rows

fit <- data_frame(
    x=1:10,
    ymed=foo(10, median(S %>% filter(Parameter=="rho") %>% pull(value)),
        median(S %>% filter(Parameter=="theta") %>% pull(value))),
    ymin=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.025),
        median(S %>% filter(Parameter=="theta") %>% pull(value))),
    ymax=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.975),
        median(S %>% filter(Parameter=="theta") %>% pull(value)))
)



bind_rows(.id="set", ref=fc0, me=fc1) %>%
    ggplot() +
    geom_col(aes(x=n, y=value, fill=set), position="dodge") +
    geom_pointrange(data=fit, aes(x, ymed, ymin=ymin, ymax=ymax)) +
#    geom_line(data=S2, aes(x=n, y=value, group=ic), alpha=.02, color="grey30") +
    geom_line(data=fit, aes(x, y=ymin), color="blue") +
    geom_line(data=fit, aes(x, y=ymax), color="blue") +
    geom_line(data=fit, aes(x, y=ymed), color="red") 


ggsave("img-freqspec-stan-fit-01.png")


## binom mic
img_bm <- stan_model("img-coalescent-binom-mix.stan")
#img_bm <- readRDS("img-coalescent-binom-mix.rds")

fc2 <- rbinoX(round(sum(fc1$value)), 1:10, .8, fc1$value, df=T)

m_img_bm <- sampling(
    img_bm,
    iter = 1000, chains=1,
    data=list(
        N=nrow(fc2), y=fc2$y, K=ng,
        sizes=1:ng, probs=rep(.8, ng)))

print(m_img_bm)
S <- ggs(m_img_bm)
ggs_traceplot(S) + facet_wrap(~Parameter, ncol=5)
ggs_caterpillar(S)

fd <- bind_rows(
    .id="set", ref=fc1,
    "p80"=fc2 %>% group_by(y) %>% summarize(n=n()) %>%
    select(value=n, n=y))

fit <- data_frame(
    x=1:10,
    ymed=foo(10, median(S %>% filter(Parameter=="rho") %>% pull(value)),
        #        median(S %>% filter(Parameter=="theta") %>% pull(value))),
        200),
    ymin=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.025),
        #        median(S %>% filter(Parameter=="theta") %>% pull(value))),
                200),
    ymax=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.975),
        #        median(S %>% filter(Parameter=="theta") %>% pull(value)))
                200)
)

fit

#    ymed=foo(10, .50, 199.95),
#    ymax=foo(10, .51, 202.95))


ggplot(data=fd) +
    geom_col(aes(x=n, y=value, fill=set), position=position_dodge(preserve = "single")) +
    geom_line(data=fit, aes(x, y=ymin), color="blue") +
    geom_line(data=fit, aes(x, y=ymax), color="blue") +
    geom_line(data=fit, aes(x, y=ymed), color="red") 


library(magrittr)

params <- data_frame(
    rho=c(0.1, 0.25, 0.1, 0.25),
    theta = c(100, 100, 200, 200))

t1 <- apply(params, 1, function(r){ z <- foo(10,r[1], r[2]); z/sum(z) }) %>%
    tbl_df %>% set_colnames(paste(params$rho, params$theta)) %>% mutate( n=row_number()) %>%
    gather(key="rho_theta", value="value", -n)

ggplot(t1) + aes(x=n, y=value, color=rho_theta) + geom_line(aes(linetype=rho_theta))


################################################################################
## Priors
fc1
x <- 0:1000
d0 <- bind_rows(
    .id="dist",
    cauchy_50_5=data_frame(x=x, y=dcauchy(x,50,5)),
    norm_50_5=data_frame(x=x, y=dnorm(x,50,5)),
    gamma_s50_m50=data_frame(x=x, y=dgamma(x,5,scale=50)),
    cauchy_500_5=data_frame(x=x, y=dcauchy(x,500,5)),
    norm_500_5=data_frame(x=x, y=dnorm(x,500,5)),
    gamma_s500_m500=data_frame(x=x, y=dgamma(x,50,scale=500))
)

x <- 0:50
d0 <- bind_rows(
    .id="dist",
    gamma_s1_t2=data_frame(x=x, y=dgamma(x,1,scale=2)),
    gamma_s2_t2=data_frame(x=x, y=dgamma(x,2,scale=2)),
    gamma_s3_t2=data_frame(x=x, y=dgamma(x,3,scale=2)),
    gamma_s1_t4=data_frame(x=x, y=dgamma(x,1,scale=4)),
    gamma_s2_t4=data_frame(x=x, y=dgamma(x,2,scale=4)),
    gamma_s3_t4=data_frame(x=x, y=dgamma(x,3,scale=4))
)

x <- 0:5000
d0 <- bind_rows(
    .id="dist",
    cauchy_1000_5=data_frame(x=x, y=dcauchy(x,1000,1000)))

x <- 0:1500/10
d0 <- bind_rows(
    .id="dist",
    cauchy_05_5=data_frame(x=x, y=dcauchy(x,0.5,10)))


ggplot(d0) + aes(x,y,color=dist) + geom_line() + ylim(0,NA)
#+END_SRC

