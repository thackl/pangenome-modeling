** Model Incompleteness as a Binomial Process on Frequency Spectrum

Each true frequency ("bar") can be modelled as its own binomial distribution (or
better something similar to a binomial distribution, but with smaller variance
in case of different completeness values for different genomes)

Use something like an MLE/EM to fit one binomial distribution foreach 1
... G. The binomial for G should be a good basis for core genome size
estimates...

Once we've established this, we can
- a) apply IMG (maybe to a range of likely estimations, given pangenome and
  supergenome size estimate ranges
- b) refine per gene probabilities of being core given individual genome
  completeness, using a conditional probability approach

*** model frequencies of incomplete data with binomial

#+BEGIN_SRC R
library(tidyverse)

# 100 genomes
# 1000 core genes

t0 <- data_frame(
    genomes = c(80:99, 100),
    count = c(rnorm(length(genomes)-1,100, 50),1000)
)

sum(t0$count)

ggplot(t0) + aes(genomes, count) + geom_col()

p <- 0.8


# this doesn't make sense, cause it samples clustered within COG, not within Genome
clustered_binomial <- function(size, min, max){
    y <- sample(min:max,1)
    sum(y*replicate(size/y, rbinom(1 ,1, runif(1, 0.6,1))))
}


t1 <- bind_rows(.id="Completeness",
                `0_p100` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=rbinom(.$count,.$genomes,1))),
                `1_p099.9` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=rbinom(.$count,.$genomes,0.999))),
                `2_p080` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=rbinom(.$count,.$genomes,0.80))),
                `3_p060-100` = t0 %>% rowwise() %>% do(data_frame(genomes=.$genomes, count=.$count, seen=replicate(.$count, sum(replicate(.$genomes, rbinom(1 ,1, runif(1, 0.6,1))))))))


t1 %>% group_by(Completeness) %>% summarize(sum(seen))

b1 <- data_frame(
    x=1:100,
    y=dbinom(x,100,.80) * 2000)

ggplot(t1) +
    aes(seen, fill=as.character(genomes)) +
    geom_bar() +
    geom_line(aes(x,y, fill=NA), b1) +
    xlim(0,101) +
    facet_wrap(~Completeness, ncol=1)

t1 %>% filter(genomes==100)

ggsave("freq-spectrum-core-binomial.png")

rbinom(1, 100, 0.8)
library(mixtools)



binom
#+END_SRC

*** fit data by Maximum Likelihood Estimation (MLE)


Solve by MLE

Binomial Likelihood & Log-Likelihood
- https://rpubs.com/felixmay/MLE
- https://sites.warnercnr.colostate.edu/gwhite/wp-content/uploads/sites/73/2017/04/BinomialLikelihood.pdf
- http://apps.usd.edu/coglab/psyc792/pdf/BinomialLikelihoodFunction.pdf
- http://shiny.stat.calpoly.edu/MLE_Binomial/
- https://somerealnumbers.wordpress.com/2015/07/06/maximum-likelihood-fitting-with-r/


Fitting a mixture model
- http://tinyheero.github.io/2016/01/03/gmm-em.html



#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)

#' @param n total number of trials
#' @param spw parameters of the mixture components as data.frame with columns
#' 'size','prob' and 'weight'
rbinoX <- function(n, sizes, probs, weights=1){
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
        mutate(weight=weight*n/sum(weight)) # this is the worst randomizer :)
    apply(spw, 1, function(r){
        rbinom(r[3], r[1], r[2])}) %>% unlist %>% as.vector
}


n <- 100
p <- 0.8
weights <- c(1,5)
weights_3 <- c(10, rep(1,19), 10)

f0 <- bind_rows(
    .id="fun",
    binom=data_frame(x=rbinom(n, 100, p)),
    binoX_1=data_frame(x=rbinoX(n, 20:50, p)),
    binoX_2=data_frame(x=rbinoX(n, c(50,80), p, weights)),
    binoX_3=data_frame(x=rbinoX(n, c(50:70), p, weights_3))
)

f1 <- f0 %>% group_by(fun, x) %>%
    summarize(y=n())

f1_gg <- ggplot(f1) + aes(x, y, color=fun, fill=fun) +
    geom_pointrange(aes(ymin=0, ymax=y), shape=22) +
    facet_wrap(~fun, ncol=1)
f1_gg

# The PMF of a mixture (for a finite set of discrete distributions) is the
# weighted sum of the component distributions f(x) = sum(w_i * p_i(x)) for
# i:=1..n
# https://en.wikipedia.org/wiki/Mixture_distribution
dbinoX_superslow <- function(x, sizes, probs, weights=1, log=FALSE){
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
            mutate(weight=weight/sum(weight)) # relative weight
    y <- sapply(x, function(k){ # for each observed value
        sum(apply(spw, 1, function(l){ # sum over each params (sizes+probs) * weight
            dbinom(k, l[1], l[2]) * l[3]
        }))
    })
    if(log==TRUE) log(y) else y
}

dbinoX_slow <- function(x, sizes, probs, weights=1, log=FALSE){
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
            mutate(weight=weight/sum(weight)) # relative weight
    y <- apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}) %>%
        rowSums
    if(log==TRUE) log(y) else y
}

dbinoX <- function(x, sizes, probs, weights=1, log=FALSE){
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}


## plot some toy data
x <- 1:n
p <- 0.8

# test functions
dbinoX_superslow(x, c(50,80), p, weights)
dbinoX_slow(x, c(50,80), p, weights)
dbinoX(x, c(50,80), p, weights)

f2 <- bind_rows(
    .id="fun",
    binom=data_frame(
        x=x,
        y=dbinom(x, 100, p) * n),
    binoX_1=data_frame(
        x=x,
        y=dbinoX(x, 20:50, p) * n),
    binoX_2=data_frame(
        x=x,
        y=dbinoX(x, c(50,80), p, weights)* n),
    binoX_3=data_frame(
        x=x,
        y=dbinoX(x, c(50:70), p, weights_3)* n)

)
f2


f2_gg <- f1_gg + geom_point(data=f2) + geom_line(data=f2, linetype=3)
f2_gg

# For the binomial distr. B(n,p), the likelihood L(theta|x) for a given
# parameter (theta) given the observed data (x), is given by the PMF of the
# binomial distr.: L(theta|x) = p_theta(x) = P_theta(X=x)
# https://en.wikipedia.org/wiki/Likelihood_function
#
# And from the mixture rule above it follows for the Likelihood of a binomial
# mixture:


LL_binoX <- function(...){
    -sum(dbinoX(..., log=TRUE))
}


# optim ######################################################################
# http://www.magesblog.com/2013/03/how-to-use-optim-in-r.html
# optimize weights
x <- f0 %>% filter(fun=="binoX_2") %>% pull(x)
sizes <- c(50,80)
probs <- p
weights_initial <- rep(1, length(sizes))
MLE <- optim(weights_initial, LL_binoX, x=x, sizes=sizes, probs=probs,
             method="L-BFGS-B", lower=0, upper=1, control=list(trace=TRUE))
MLE
(weights_mle <- MLE$par / sum(MLE$par))


x_3 <- f0 %>% filter(fun=="binoX_3") %>% pull(x)
sizes_3 <- c(50:70)
probs <- p
weights_initial_3 <- rep(1, length(sizes_3))

MLE <- optim(weights_initial_3, LL_binoX, x=x_3, sizes=sizes_3, probs=probs,
             method="L-BFGS-B", lower=0, upper=1, control=list(trace=TRUE))
MLE
(weights_mle_3 <- MLE$par / sum(MLE$par))


LL_binoX(x_3, sizes_3, probs, weights_initial_3)
LL_binoX(x_3, sizes_3, probs, weights_3)
LL_binoX(x_3, sizes_3, probs, weights_mle_3)


## ?? Might be worth looking into
MLE3 <- constrOptim(weights_initial, LL_binoX, x=x, sizes=sizes, probs=probs,
             method="L-BFGS-B", ui)


# mle2 ######################################################################
library(bbmle)

ll <- function(w1, w2){
    -sum(dbinoX(x, sizes, probs, weights=c(w1, w2), log=TRUE))
}

MLE2 <- mle2(
    ll, method="L-BFGS-B",
    lower=c(w1=0, w2=0),
    upper=c(w1=1, w2=1),
    start=list(w1=0.5,w2=0.5),
    control=list(trace=TRUE))
MLE2
(weights_mle2 <- coef(MLE2) / sum(coef(MLE2)))

MLE2 <- mle2(
    ll, method="L-BFGS-B",
    lower=c(w1=0, w2=0),
    upper=c(w1=1, w2=1),
    start=list(w1=0.5,w2=0.5),
    control=list(trace=TRUE))
MLE2
(weights_mle2 <- coef(MLE2) / sum(coef(MLE2)))


MLE
MLE2

# plot ######################################################################
x <- 1:n
f3 <- bind_rows(
    .id="fun",
#    binoX_1=data_frame(
#        x=x,
#        y=dbinoX(x, 20:50, p, weights_mle) * n),
    binoX_2=data_frame(
        x=x,
        y=dbinoX(x, c(50,80), p, weights_mle2)* n),
    binoX_3=data_frame(
        x=x,
        y=dbinoX(x, sizes_3, p, weights_mle_3)* n)
)

f2_gg + geom_point(data=f3, shape=5) + geom_line(data=f3, linetype=5, color="blue")


# MLE standard error / cofidence intervals
# https://stats.stackexchange.com/questions/27033/in-r-given-an-output-from-optim-with-a-hessian-matrix-how-to-calculate-paramet



################################################################################
## ************************************************************************** ##
################################################################################
## plot pangenome data
p <- 0.8
weights <- c(1000, rnbinom(98,10,mu=100), 1000) # unnormalized weights (sum != 1)
n <- sum(weights) # total genes
sizes <- 1:100

p0 <- bind_rows(
    .id="fun",
    p100=data_frame(x=rbinoX(n, sizes, 1, weights)),
    p80=data_frame(x=rbinoX(n, sizes, p, weights))
)
p1 <- p0 %>% group_by(fun, x) %>%
    summarize(y=n())

p1_gg <- ggplot(p1) + aes(x, y, color=fun, fill=fun) +
    geom_pointrange(aes(ymin=0, ymax=y), shape=22) +
    facet_wrap(~fun, ncol=1)
p1_gg

x <- p0 %>% filter(fun=="p80") %>% pull(x)
weights_initial <- rep(1, length(sizes))


## optim
MLE <- optim(weights_initial, LL_binoX, x=x, sizes=sizes, probs=p,
             method="L-BFGS-B", lower=0, upper=1)
MLE

MLE <- optim(rep(1, 11), LL_binoX, x=x, sizes=c(90:100), probs=p,
             method="L-BFGS-B", lower=0.01)
MLE



LL_binoX(x=x, sizes=sizes, probs=p, weights_initial)


(weights_mle <- MLE$par / sum(MLE$par))

x <- 1:n
f3 <- bind_rows(
    .id="fun",
#    binoX_1=data_frame(
#        x=x,
#        y=dbinoX(x, 20:50, p, weights_mle) * n),
    binoX_2=data_frame(
        x=x,
        y=dbinoX(x, c(50,80), p, weights_mle)* n)
)

f2_gg + geom_point(data=f3, shape=5) + geom_line(data=f3, linetype=5, color="blue")



################################################################################


rLL_binoX <- function(x, sizes, probs, weights, lambda=1e2, minimum=NULL){
    # only weights > minimum
    if(!is.null(minimum) & any(weights<minimum)) return(Inf)

    # regularized negative log likelihood
    -sum(dbinoX(x, sizes, probs, weights, log=TRUE)) +
        # regularize weights to sum to 1
        lambda * (sum(abs(weights))-1)
}

sum(weights_initial_3)

w4 <- weights_initial_3 / sum(weights_initial_3)

LL_binoX(x_3, sizes_3, probs, weights_initial_3)
rLL_binoX(x_3, sizes_3, probs, weights=weights_initial_3)
rLL_binoX(x_3, sizes_3, probs, weights=weights_initial_3, lambda=10)
rLL_binoX(x_3, sizes_3, probs, weights=w4, lambda=10)
rLL_binoX(x_3, sizes_3, probs, -w4, 10)


freq_sim <- function(d, n=1){
    d$sim <- list()
    for(i in 1:n){
        d$sim[[i]] <- rbinoX(sum(d$true$freq), d$true$size, d$p, d$true$freq) %>%
            table %>% as_data_frame %>%
            select(size=1, freq=2) %>%
            mutate(size=as.numeric(size))
    }
    d
}

freq_fit <- function(d, regularized=FALSE){
    for(i in 1:length(d$sim)){
        # fit mix with same number of components as in truth
        weights_guess <- c(rep(1, length(d$true$size)-2), 10,100) # mostly core
        weights_guess <- weights_guess/sum(weights_guess)
        x <- rep(d$sim[[i]]$size, d$sim[[i]]$freq)
        sizes <- d$true$size
        n <- sum(d$true$freq) # TODO: n when truth unknown?

        if(regularized){
            MLE <- optim(
                weights_guess, rLL_binoX,
                method="Nelder-Mead", control=list(trace=TRUE, maxit=500),
                x=x, sizes=sizes, probs=d$p, minimum=0) # data
        }else{
            MLE <- optim(
                weights_guess, rLL_binoX,
                method="L-BFGS-B", lower=0, control=list(trace=TRUE, maxit=500),
                x=x, sizes=sizes, probs=d$p) # data
        }

        weights_mle <- MLE$par / sum(MLE$par) # normalized weights
        print(weights_mle)

        d$fit[[i]] <- data_frame(
            size=d$range,
            freq=dbinoX(d$range, sizes, p, weights_mle) * n)
        d$est[[i]] <- data_frame(
            size=sizes,
            freq=weights_mle * n)
    }
    d
}

#    freq_true = c(1000, rnbinom(98,10,mu=100), 1000) # =~ unnormalized weights

d <- list(
    p=0.8,
    true = data_frame(
        size = c(981:1000),
        freq = c(rep(100, 18),200,1000)))
d$range <- 1:max(d$true$size)

d <- freq_sim(d, 3)

d <- freq_fit(d, regularized=TRUE)

dev.new()
d1 <- bind_rows(
    .id="fit",
    true=d$true %>% mutate(i="0"),
    est=bind_rows(.id="i", d$est),
    sim=bind_rows(.id="i", d$sim),
    fit=bind_rows(.id="i", d$fit)
)
ggplot(d1) + aes(x=size, y=freq, fill=fit, color=fit) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept=1000) +
    xlim(700,NA) +
#    geom_density(stat="identity", alpha=.2) +
    facet_wrap(~i)


# plot
# - true spectrum
# - observed spectrum

d
gg <- ggplot(dx) + aes(x) + geom_bar(aes(group=size, fill=as.factor(size))) +
    scale_fill_manual(values=rep(brewer.pal(10,"Spectral"), times=10)) +
    facet_wrap(~par)
gg
#+END_SRC

*** regularized MLE with confidence intervals

#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)
library(bbmle)

#' @param n total number of trials
#' @param spw parameters of the mixture components as data.frame with columns
#' 'size','prob' and 'weight'
rbinoX <- function(n, sizes, probs, weights=NULL){
    if(is.null(weights)) weights <- 1
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
        mutate(weight=weight*n/sum(weight)) # this is the worst randomizer :)
    apply(spw, 1, function(r){
        rbinom(r[3], r[1], r[2])}) %>% unlist %>% as.vector
}

dbinoX <- function(x, sizes, probs, weights=NULL, log=FALSE){
    if(is.null(weights)) weights <- 1
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}

LL_binoX <- function(x, sizes, probs, weights, lambda=NULL, lower_bound=NULL){
    # penalize weights < lower_bound bound
    if(!is.null(lower_bound) && any(weights<lower_bound)) return(1e10)
    # negative log likelihood
    ll <- -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
    # regularize weights to sum to 1
    if(!is.null(lambda)) ll <- ll + lambda * (sum(abs(weights))-1)
    ll
}

data_frame(x = rbeta(10000, 1,1)) %>% ggplot() + aes(x) + geom_density()

d <- data_frame(x=1:100, y=dbeta(1:100/100,30,1))
ggplot(d) + aes(x,y) + geom_point()


LL_beta_binoX <- function(x, sizes, probs, alpha, beta, lambda=NULL){
    # get weights from dbeta
    m <- length(sizes)
    weights <- dbeta(1:m/m,alpha,beta)
    # negative log likelihood
    ll <- -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
    # regularize weights to sum to 1
    if(!is.null(lambda)) ll <- ll + lambda * (sum(abs(weights))-1)
    ll
}


freq_sim <- function(d, n=1){
    d$sim <- list()
    for(i in 1:n){
        d$sim[[i]] <- rbinoX(sum(d$true$freq), d$true$size, d$p, d$true$freq) %>%
            table %>% as_data_frame %>%
            select(size=1, freq=2) %>%
            mutate(size=as.numeric(size))}
    d
}

freq_fit <- function(d, wrapper="optim", method="Nelder-Mead", lambda=NULL, lower_bound=NULL, control){
    d$mle <- list()
    d$fit <- list()
    d$est <- list()
    for(i in 1:length(d$sim)){
        # fit mix with same number of components as in truth
        weights_guess <- c(rep(1, length(d$true$size)-2), 10,100) # mostly core
        weights_guess <- weights_guess/sum(weights_guess)
        x <- rep(d$sim[[i]]$size, d$sim[[i]]$freq)
        sizes <- d$true$size
        n <- sum(d$true$freq) # TODO: n when truth unknown?

        if(is.null(wrapper) || wrapper=="optim"){
            MLE <- optim(
                weights_guess, LL_binoX,
                method=method, control=control,
                # ll params
                lambda=lambda, lower_bound=lower_bound,
                # ll data
                x=x, sizes=sizes, probs=d$p)

            weights_mle <- MLE$par / sum(MLE$par) # normalized weights
        }else if(wrapper=="mle2"){
            # make optim-style params compatible with mle2 interface
            weights_guess_names <- paste0("w", 1:length(weights_guess))
            names(weights_guess) <- weights_guess_names
            parnames(LL_binoX) <- weights_guess_names

            MLE <- mle2(
                LL_binoX, method=method,
                start=weights_guess,
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, lambda=lambda, lower_bound=lower_bound))

            weights_mle <- coef(MLE) / sum(coef(MLE)) # normalized weights
        }else if(wrapper=="mle2-beta"){
            MLE <- mle2(
                LL_beta_binoX, method=method,
                start=list(alpha=30, beta=1),
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, lambda=lambda, lower_bound=lower_bound))

            m <- length(sizes)
            ab <- coef(MLE) # estimated alpha and beta from BETA
            weights_mle <- dbeta(1:m/m,ab[1],ab[2])
            weights_mle <- weights_mle/sum(weights_mle) # normalize weights
        }

        d$mle[[i]] <- MLE
        d$fit[[i]] <- data_frame(
            size=d$range,
            freq=dbinoX(d$range, sizes, p, weights_mle) * n)
        d$est[[i]] <- data_frame(
            size=sizes,
            freq=weights_mle * n)
    }
    d
}

#    freq_true = c(1000, rnbinom(98,10,mu=100), 1000) # =~ unnormalized weights

d0 <- list(
    p=0.8,
    true = data_frame(
        size = c(981:1000),
        freq = c(rep(100, 18),200,1000)))
d0$range <- 1:max(d0$true$size)

d1 <- freq_sim(d0, 3)

#d2 <- freq_fit(d1, "mle2", method="Nelder-Mead", lambda=100, lower_bound=0, control=#list(trace=TRUE, maxit=1e4))

d2 <- freq_fit(d1, "mle2-beta", method="Nelder-Mead", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e4))

#d2 <- freq_fit(d1, "optim", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e2))

d3 <- bind_rows(
    .id="fit",
    true=d2$true %>% mutate(i="0"),
    est=bind_rows(.id="i", d2$est),
    sim=bind_rows(.id="i", d2$sim),
    fit=bind_rows(.id="i", d2$fit)
)
ggplot(d3) + aes(x=size, y=freq, fill=fit, color=fit) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept=1000) +
    xlim(700,NA) +
#    geom_density(stat="identity", alpha=.2) +
    facet_wrap(~i)

p1 <- profile(d2$mle[[1]], skip.hessian=TRUE)
p2 <- profile(p1, std.err=2.404e+04)
confint(p2)
coef(p1)

plot(p2, absVal=FALSE)

ci <- confint(d2$mle[[1]])

coef(d2$mle[[1]])
coef(ci)

# plot
# - true spectrum
# - observed spectrum

d
gg <- ggplot(dx) + aes(x) + geom_bar(aes(group=size, fill=as.factor(size))) +
    scale_fill_manual(values=rep(brewer.pal(10,"Spectral"), times=10)) +
    facet_wrap(~par)
gg

# my hessian can't be inverted, so profiling and confidence intervals aren't available..










x <- 0:10
y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8)
d <- data.frame(x,y)
## in general it is best practice to use the data argument,
##  but variables can also be drawn from the global environment
LL <- function(ymax=15, xhalf=6)
-sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
## uses default parameters of LL
(fit <- mle2(LL))
fit1F <- mle2(LL, fixed=list(xhalf=6))

coef(fit1F)
coef(fit1F,exclude.fixed=TRUE)
(fit0 <- mle2(y~dpois(lambda=ymean),start=list(ymean=mean(y)),data=d))
anova(fit0,fit)
summary(fit)
logLik(fit)
vcov(fit)
p1 <- profile(fit)
plot(p1, absVal=FALSE)
confint(fit)

## use bounded optimization
## the lower bounds are really > 0, but we use >=0 to stress-test
## profiling; note lower must be named
(fit1 <- mle2(LL, method="L-BFGS-B", lower=c(ymax=0, xhalf=0)))
p1 <- profile(fit1)
plot(p1, absVal=FALSE)
## a better parameterization:
LL2 <- function(lymax=log(15), lxhalf=log(6))
-sum(stats::dpois(y, lambda=exp(lymax)/(1+x/exp(lxhalf)), log=TRUE))
(fit2 <- mle2(LL2))
plot(profile(fit2), absVal=FALSE)
exp(confint(fit2))
vcov(fit2)
cov2cor(vcov(fit2))
#+END_SRC

*** MLE on normalized weights
Gabe's idea: change of variable:

wi => xi
xi = wi/w1
x1 = w1/w1 = 1 = const (edited)
Dann variieren alle xi zwischen [0,inf)

Gff Dann noch weiter
yi = log(xi)
y1 = 0 = const
Dann variieren alle y zwischen (-inf,inf)
Ausser y1=0
Dann musst du nur bei der likelihood calculation jeweils die y in w zurückrechnen
wi = exp(yi) * w1
Wobei w1 = 1 - sum(w2...wn)
Das ist ein lineares system
Kannst du mit 'solve' lösen

(Kann sein, dass da noch der Wurm drin ist - hab's ohne solve gemacht...)
=> *Konvergiert schneller (Nelder-Mead < 1000), aber Ergebnisse sind nicht besser*

#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)
library(bbmle)

#' @param n total number of trials
#' @param spw parameters of the mixture components as data.frame with columns
#' 'size','prob' and 'weight'
rbinoX <- function(n, sizes, probs, weights=NULL){
    if(is.null(weights)) weights <- 1
    spw <- data_frame(size=sizes, prob=probs, weight=weights) %>%
        mutate(weight=weight*n/sum(weight)) # this is the worst randomizer :)
    apply(spw, 1, function(r){
        rbinom(r[3], r[1], r[2])}) %>% unlist %>% as.vector
}

dbinoX <- function(x, sizes, probs, weights=NULL, log=FALSE){
    if(is.null(weights)) weights <- 1
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}

LL_binoX <- function(x, sizes, probs, weights, lambda=NULL, lower_bound=NULL){
    # penalize weights < lower_bound bound
    if(!is.null(lower_bound) && any(weights<lower_bound)) return(1e10)
    # negative log likelihood
    ll <- -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
    # regularize weights to sum to 1
    if(!is.null(lambda)) ll <- ll + lambda * (sum(abs(weights))-1)
    ll
}

w2x <- function(weights) log(weights[2:length(weights)]/weights[1])
x2w <- function(w1, xeights) c(w1, exp(xeights) *w1)

LL_x_binoX <- function(x, sizes, probs, w1, xeights, lower_bound=NULL){
    weights <- x2w(w1, xeights)
    # negative log likelihood
    -sum(dbinoX(x, sizes, probs, weights, log=TRUE))
}

freq_sim <- function(d, n=1){
    d$sim <- list()
    for(i in 1:n){
        d$sim[[i]] <- rbinoX(sum(d$true$freq), d$true$size, d$p, d$true$freq) %>%
            table %>% as_data_frame %>%
            select(size=1, freq=2) %>%
            mutate(size=as.numeric(size))}
    d
}

freq_fit <- function(d, wrapper="optim", method="Nelder-Mead", lambda=NULL, lower_bound=NULL, control){
    d$mle <- list()
    d$fit <- list()
    d$est <- list()
    for(i in 1:length(d$sim)){
        # fit mix with same number of components as in truth
        weights_guess <- c(rep(1, length(d$true$size)-2), 10,100) # mostly core
        weights_guess <- weights_guess/sum(weights_guess)
        x <- rep(d$sim[[i]]$size, d$sim[[i]]$freq)
        sizes <- d$true$size
        n <- sum(d$true$freq) # TODO: n when truth unknown?

        if(is.null(wrapper) || wrapper=="optim"){
            MLE <- optim(
                weights_guess, LL_binoX,
                method=method, control=control,
                # ll params
                lambda=lambda, lower_bound=lower_bound,
                # ll data
                x=x, sizes=sizes, probs=d$p)

            weights_mle <- MLE$par / sum(MLE$par) # normalized weights
        }else if(wrapper=="mle2"){
            # make optim-style params compatible with mle2 interface
            weights_guess_names <- paste0("w", 1:length(weights_guess))
            names(weights_guess) <- weights_guess_names
            parnames(LL_binoX) <- weights_guess_names

            MLE <- mle2(
                LL_binoX, method=method,
                start=weights_guess,
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, lambda=lambda, lower_bound=lower_bound))

            weights_mle <- coef(MLE) / sum(coef(MLE)) # normalized weights
        }else if(wrapper=="mle2-x"){
            # make optim-style params compatible with mle2 interface
            w1 <- weights_guess[1]
            xeights_guess <- w2x(weights_guess)
            xeights_guess_names <- paste0("x", 1:length(xeights_guess))
            names(xeights_guess) <- xeights_guess_names
            parnames(LL_x_binoX) <- xeights_guess_names

            MLE <- mle2(
                LL_x_binoX, method=method,
                start=xeights_guess,
                control=control,
                data=list(x=x, sizes=sizes, probs=d$p, w1=w1, lower_bound=lower_bound))

            weights_mle <- x2w(w1, coef(MLE))
            weights_mle <- weights_mle/sum(weights_mle) # normalized weights
        }

        d$mle[[i]] <- MLE
        d$fit[[i]] <- data_frame(
            size=d$range,
            freq=dbinoX(d$range, sizes, d$p, weights_mle) * n)
        d$est[[i]] <- data_frame(
            size=sizes,
            freq=weights_mle * n)
    }
    d
}

#    freq_true = c(1000, rnbinom(98,10,mu=100), 1000) # =~ unnormalized weights

d0 <- list(
    p=0.8,
    true = data_frame(
        size = c(981:1000),
        freq = c(rep(100, 18),200,1000)))
d0$range <- 1:max(d0$true$size)

d1 <- freq_sim(d0, 3)

#d2 <- freq_fit(d1, "mle2", method="Nelder-Mead", lambda=100, lower_bound=0, control=#list(trace=TRUE, maxit=1e4))

d2 <- freq_fit(d1, "mle2-x", method="Nelder-Mead", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e4))

#d2 <- freq_fit(d1, "optim", lambda=100, lower_bound=0, control=list(trace=TRUE, maxit=1e2))

d3 <- bind_rows(
    .id="fit",
    true=d2$true %>% mutate(i="0"),
    est=bind_rows(.id="i", d2$est),
    sim=bind_rows(.id="i", d2$sim),
    fit=bind_rows(.id="i", d2$fit)
)
ggplot(d3) + aes(x=size, y=freq, fill=fit, color=fit) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept=1000) +
    xlim(700,NA) +
#    geom_density(stat="identity", alpha=.2) +
    facet_wrap(~i)

d2$mle[[1]]

p1 <- profile(d2$mle[[1]])
p2 <- profile(p1, std.err=2.403e+04)
confint(p1)
coef(p1)

plot(p2, absVal=FALSE)

ci <- confint(d2$mle[[1]])

coef(d2$mle[[1]])
coef(ci)
#+END_SRC

** Bayesian Inference
*** Background
Really useful intro video on how to apply bayesian data analysis/inference

- Part 1: https://www.youtube.com/watch?v=3OJEae7Qb_o
  - exercise: https://goo.gl/cxfnYK (or http://www.sumsar.net/files/posts/2017-bayesian-tutorial-exercises/modeling_exercise1.html)
- Part 2: https://www.youtube.com/watch?v=mAUwjSo5TJE
- Part 3: https://www.youtube.com/watch?v=Ie-6H_r7I5A
  - Stan


Books:
- "Doing Bayesian Data Analysis", John K. Kruschke
- "Statistical Rethinking", Richard McElreath

*** Stan/rstan examples
**** "8schools"

#+BEGIN_SRC R
library("rstan") # observe startup messages
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

stan_model_8schools <- "
// saved as 8schools.stan
data {
  int<lower=0> J; // number of schools
  real y[J]; // estimated treatment effects
  real<lower=0> sigma[J]; // s.e. of effect estimates
}
parameters {
  real mu;
  real<lower=0> tau;
  real eta[J];
}
transformed parameters {
  real theta[J];
  for (j in 1:J)
    theta[j] = mu + tau * eta[j];
}
model {
  target += normal_lpdf(eta | 0, 1);
  target += normal_lpdf(y | theta, sigma);
}
"

schools_dat <- list(J = 8,
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))

fit <- stan(model_code=stan_model_8schools, data = schools_dat,
            iter = 1000, chains = 4)

print(fit)
plot(fit)
#+END_SRC

**** Mike Lawrence tutorial
- Video: https://www.youtube.com/watch?v=ev2xpOKxbDQ&index=1&list=PLu77iLvsj_GPoC6tTw01EP1Tcr2I6zEm8

#+BEGIN_SRC R
#one group, known error variance

#clear workspace to ensure a fresh start
rm(list=ls())

#load rstan
library(rstan)

#load ggmcmc
library(ggmcmc)

#set the random seed (so we all generate the same fake data)
set.seed(1)

#generate some fake data
Y <- rnorm(1e4,100,15)

#package the data for stan
data <- list(
	nY = length(Y)
	, Y = Y
)

stan_model <- '
// one group, known error variance
data {
  int nY ; // initialize a variable to indicate the number of elements in Y
  vector[nY] Y ; // initialize a vector to hold the observations
}
parameters { // what we want to infer
  real mu ; // mean of the data-generating population
}
model {
  // priors
  mu ~ normal(100,20) ;

  // generator
  Y ~ normal(mu,15);
}
'

model <- rstan::stan_model(model_code=stan_model)

#evaluate the model
sampling_iterations <- 1e4 #best to use 1e3 or higher
out <- rstan::sampling(
	object = model
	, data = data
	, chains = 1
	# , chains = 4
	# , cores = 4
	, iter = sampling_iterations
	, warmup = sampling_iterations/2
	, refresh = sampling_iterations/10 #show an update @ each %10
	, seed = 1
)

#print a summary table
print(out)
#look at n_eff and RHat to see if we've sampled enough
#    we generally want n_eff>1e3 & Rhat==1
#    if these criteria are not met, run again with more iterations


#extract the posterior groups in a format that ggmcmc likes
samples <- ggmcmc::ggs(out)

#show the histogram of sampled mu values
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_histogram'
)


#look at the traceplot (should look like white noise)
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_traceplot'
)

#look at the full-vs-partial density (should look the same)
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_compare_partial'
)

#look at the auto-correlations
ggmcmc::ggmcmc(
	D = samples
	, file = NULL
	, plot = 'ggs_autocorrelation'
)
library(shinystan)
shinystan::launch_shinystan(out)
#+END_SRC

**** Hierarchical Logistic Regression from the Stan Manual
Guide, U. ’s, & Manual, R. (n.d.). Stan Modeling Language.
https://paperpile.com/app/p/a53d0165-7672-07dd-a014-9a99a334d1fc
[[chrome-extension://bomfdkbfpdhijjbeoicnfhjbdhncfhig/view.html?mp=ePqCTTS3]] p138

This is not exactly what I need, but it might be useful - it defines a vector of
parameters to be estimated for a series of classes, and some dynamic prior
assignment to these parameters - so quite similar to the weights I am interested
in

**** Finite mixture models in Stan
http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html


#+BEGIN_SRC R
library(dplyr);
library(ggplot2);
library(ggmcmc);

# Number of data points
N <- 400

# Let's make three states
mu <- c(3, 6, 9)
sigma <- c(2, 4, 3)

# with probability
Theta <- c(.5, .2, .3)

# Draw which model each belongs to
z <- sample(1:3, size = N, prob = Theta, replace = T)

# Some white noise
epsilon <- rnorm(N)

# Simulate the data using the fact that y ~ normal(mu, sigma) can be
# expressed as y = mu + sigma*epsilon for epsilon ~ normal(0, 1)
y <- mu[z] + sigma[z]*epsilon

data_frame(y, z = as.factor(z)) %>%
  ggplot(aes(x = y, fill = z)) +
  geom_density(alpha = 0.3)

model_stan_code <- '
data {
  int N;
  vector[N] y;
  int n_groups;
}
parameters {
  vector[n_groups] mu; // use ordered to fix label switching
  vector<lower = 0>[n_groups] sigma;
  simplex[n_groups] Theta;
}
model {
  vector[n_groups] contributions;
  // priors
  mu ~ normal(0, 10);
  sigma ~ cauchy(0, 2);
  Theta ~ dirichlet(rep_vector(2.0, n_groups));


  // likelihood
  for(i in 1:N) {
    for(k in 1:n_groups) {
      contributions[k] = log(Theta[k]) + normal_lpdf(y[i] | mu[k], sigma[k]);
    }
    target += log_sum_exp(contributions);
  }
}'

model_stan_compiled <- stan_model(model_code=model_stan_code)

model <- sampling(
    model_stan_compiled,
    data = list(N= N, y = y, n_groups = 3),
    iter = 600)

S <- ggs(model)
ggs_traceplot(S)
ggs_caterpillar(S)
#+END_SRC

*** Old Faithful bimodal with Dirichelet Mixture (python though)
http://austinrochford.com/posts/2016-02-25-density-estimation-dpm.html

*** Bayesian modeling of a binomial mixture using Stan

#+BEGIN_SRC R
library(tidyverse)
library(RColorBrewer)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

library(ggmcmc)


#' @param n total number of observations
#' @param sizes number of trials for each component (single value will be recycled)
#' @param probs probabilities of success for each component (single value will be recycled)
#' @param weights of components, i.e. proportion of observation from total. Values will be normalized to sum to 1. Single values will be recycled.
#' 'size','prob' and 'weight'
#' @param df if TRUE, return data.frame including parameters used to simulate
#' each outcome.
rbinoX <- function(n, sizes, probs, weights=NULL, df=FALSE){
    if(is.null(weights)) weights <- 1
    cns <- sapply(list(sizes, probs, weights), length)
    cn <- max(cns)

    if(any(cns!=cn & cns!=1)) stop("sizes, probs and weights lengths differ. Need to either be the same or 1 (recycling)")

    if(length(sizes)==1) sizes <- rep(sizes, cn)
    if(length(probs)==1) probs <- rep(probs, cn)
    if(length(weights)==1) weights <- rep(weights, cn)

    r <- t(sapply(sample(1:cn, n, replace=TRUE, prob=weights), function(ci){
        c(sizes[ci], probs[ci], weights[ci], rbinom(1, sizes[ci], probs[ci]))}))
    if(df){
        r <- as.data.frame(r)
        colnames(r) <- c("size", "prob","weight","y")
        r
    }else{r[,4]}
}

dbinoX <- function(x, sizes, probs, weights=NULL, log=FALSE){
    if(is.null(weights)) weights <- 1
    spw <- cbind(sizes, probs, weights)
    spw[,3]=spw[,3]/sum(spw[,3]) # normalize weights
    y <- rowSums(apply(spw, 1, function(r){dbinom(x, r[1], r[2]) * r[3]}))
    if(log==TRUE) log(y) else y
}

set.seed(1337)
n <- 1e3
sizes <- c(81:100)
p <- .8
weights <- c(1:20)
weights <- weights/sum(weights)
sizes
weights

d1 <- rbinoX(n, sizes, p, weights, df=T) %>% tbl_df
d1

theme_set(theme_bw())

ggplot(d1) + aes(x=y, fill=as.factor(size)) +
#    geom_area(aes(color=as.factor(size), ),alpha=.1, stat="bin", binwidth=1, position="dodge") +
    geom_bar() +
    xlim(0,100)


#package the data for stan
data <- list(
    # observations
    N = length(d1$y), # n/o observations
    y = d1$y,
    # mixture components
    K = length(sizes), # n/o components
    sizes = sizes,
    probs = rep(p, length(sizes)),
    # priors
    phi = weights/sum(weights), # maximum knowledge
    kappa = 100 # strength
)

model_stan_compiled <- stan_model("binom-mix-repara-dirichlet.stan")

model <- sampling(
    model_stan_compiled,
    data = data,
    iter = 500, chains=4)

model_k1000 <- model
model_k100 <- model

print(model)
S <- ggs(model) %>% filter(grepl("theta", Parameter))
ggs_traceplot(S) + facet_wrap(~Parameter, ncol=5)

ggs_caterpillar(S) + geom_point(
    aes(x=x, y=Parameter),
    data=data_frame(x=weights, Parameter=unique(S$Parameter)),
    color="red", size=6, shape=124)
ggsave("Weight-caterpillar.png")

S2 <- S %>% group_by(Parameter) %>% summarize(
    mean=mean(value),
    q025=quantile(value,.025),
    q975=quantile(value,.975),
    q25=quantile(value,.25),
    q75=quantile(value,.75)) %>% ungroup

ggplot(S2) + aes(x=Parameter, y=mean, group=NA) +
    geom_ribbon(aes(ymin=q025, ymax=q975), alpha=.1) +
    geom_ribbon(aes(ymin=q25, ymax=q75), alpha=.3) +
    geom_line(color="blue", size=1, linetype=3, alpha=.8) +
    geom_point(
        aes(y=x),
        data=data_frame(x=weights, Parameter=unique(S$Parameter)),
        color="red", size=2)
ggsave("Weight-fit.png")


ggs_autocorrelation(S)
ggsave("Weight-autocorrelation.png")
ggs_crosscorrelation(S)
ggsave("Weight-crosscorrelation.png")

# Reparameterization of Dirichlet to formulate priors
# see Reparameterization of Dirichlet Priors (Stan Manual p282)
# https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E8%81%9A%E7%B1%BB/LDA/docs/dirichlet.pdf

require(MCMCpack)
library(ade4)

phi <- c(.5,.1,.1)
phi <- phi/sum(phi)
kappa <- c(10, 10, 10)
(alpha_rp <- kappa * phi)
dr <- MCMCpack::rdirichlet(1000, alpha_rp) %>% tbl_df
dr
triangle.plot(dr, min3=c(0,0,0), max3=c(1,1,1))
#+END_SRC

*** REJC Simulate data from Stan - doesn't work with mixture models that only increment likelyhood desities
    CLOSED: [2017-10-24 Tue 14:56]

#+BEGIN_SRC R
library(rstan)
library(tidyverse)

sm <- "
parameters {
  real y;
}
model {
  y ~ normal(1,1);
}
"

stan(model_code=sm)



scode <- "
     parameters {
       real y[2];
     }
     model {
       y[1] ~ normal(0, 1);
       y[2] ~ double_exponential(0, 2);
     }
     "
fit1 <- stan(model_code = scode, iter = 10, verbose = FALSE)
print(fit1)
fit2 <- stan(fit = fit1, iter = 10000, verbose = FALSE)

## extract samples as a list of arrays
e2 <- extract(fit2, permuted = TRUE)
## using as.array on the stanfit object to get samples
a2 <- as.array(fit2)
a2 %>% tbl_df
#+END_SRC

*** REJC IMG in Stan
    CLOSED: [2017-11-07 Tue 15:17]
*This doesn't work* - my approach of estimating a mixed model using some form of
IMG-derived prior doesn't work, because - I don't really know. But it is clear
that the sequential binomial mixture models cannot be robustly fitted, and rho
(& theta) values for incomplete data are always off.

The biggest problem with robustly modeling the true frequency spectrum from
incomplete data via Bayesian Inference is the choice and parameterization of the
priors. Ideally, one would like to estimate the IMG gene gain/loss parameter
directly, and mixing rates of the underlying binomials only indirectly.

**** Estimating rho and weights, lacks theta - scaling param that is non-identifiable from normalized spectra
IMG:
- gain rate :: mu
- loss rate :: nu
- N :: population size
in scaled coalescent units:
- gain rate :: theta = 2 N mu
- loss rate :: rho = 2 N nu

#+BEGIN_SRC R
library(tidyverse)
theme_set(theme_bw())
library(rstan)
library(ggmcmc)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

#' @param n total number of observations
#' @param sizes number of trials for each component (single value will be recycled)
#' @param probs probabilities of success for each component (single value will be recycled)
#' @param weights of components, i.e. proportion of observation from total. Values will be normalized to sum to 1. Single values will be recycled.
#' 'size','prob' and 'weight'
#' @param df if TRUE, return data.frame including parameters used to simulate
#' each outcome.
rbinoX <- function(n, sizes, probs, weights=NULL, df=FALSE){
    if(is.null(weights)) weights <- 1
    cns <- sapply(list(sizes, probs, weights), length)
    cn <- max(cns)

    if(any(cns!=cn & cns!=1)) stop("sizes, probs and weights lengths differ. Need to either be the same or 1 (recycling)")

    if(length(sizes)==1) sizes <- rep(sizes, cn)
    if(length(probs)==1) probs <- rep(probs, cn)
    if(length(weights)==1) weights <- rep(weights, cn)

    r <- t(sapply(sample(1:cn, n, replace=TRUE, prob=weights), function(ci){
        c(sizes[ci], probs[ci], weights[ci], rbinom(1, sizes[ci], probs[ci]))}))
    if(df){
        r <- as.data.frame(r)
        colnames(r) <- c("size", "prob","weight","y")
        r
    }else{r[,4]}
}


## Collins, R. E., & Higgs, P. G. (2012). Testing the infinitely many genes model for the evolution of the bacterial core genome and pangenome. Molecular Biology and Evolution, 29(11), 3413–3425.
source("../pangenome-infinitely-many-genes/f-pangenome.R")

# rho1, theta1, core, rho2, theta2
rho1 <- .5    # rate of loosing a single gene with time units 2N_e
theta1 <- 200  # average number of genes gained in 2N_e
ng <- 10
f.coalescent(c(rho1, theta1), ng) %>%
    tbl_df %>%
    mutate(n=1:ng) %>%
    gather(set, value, -n) %>%
#    mutate(value=00) %>%
    ggplot() + aes(x=n, y=value, color=set) + geom_line()

fc0 <- f.coalescent.spec(c(rho1, theta1), ng) %>% tbl_df %>% mutate(n=1:ng)
ggplot(fc0) + aes(x=n, y=value) + geom_col()


## the relation of theta, rho, ng, and total number of individual genes
ng=15
rho=0.05
theta=1300
theta*ng/rho
cbind(1:ng, foo(ng, rho, theta)) %>% apply(1, prod) %>% sum


##---------------------------------------------------

foo <- function(K, rho1, theta1) {

  spec <- (1:K)*0

  (specprod1 <- (K+1 - 1:K)/(K+rho1 - 1:K))

  for (k in 1:K) {
      spec[k] <- (theta1/k)*prod(specprod1[1:k])
  }

  return(spec)
}

fc1 <- foo(ng, rho1, theta1) %>% tbl_df %>% mutate(n=1:ng)
bind_rows(.id="set", ref=fc0, me=fc1) %>%
    ggplot() + aes(x=n, y=value, fill=set) + geom_col(position="dodge")


fc0
## model_stan_compiled <- stan_model("foo.stan")
## model <- sampling(
##    model_stan_compiled,
##     algorithm="Fixed_param",
##     iter = 1, chains=1, data=list(K=ng, rho=rho1, theta=theta1))


## THIS MODEL READS a frequency spectrum of gene family sizes, not the raw sizes
## for each family!!!
model_stan_compiled <- stan_model("img-freqspec-coalescent.stan")
#model_stan_compiled <- readRDS("img-freqspec-coalescent.rds")

m_img_coal_1 <- sampling(
    model_stan_compiled,
    iter = 1000, chains=4,
    data=list(
        N=nrow(fc1), y=fc1$value, K=ng))


model <- m_img_coal_1
print(model)
S <- ggs(model)
ggs_traceplot(S) + facet_wrap(~Parameter, ncol=5)
ggs_caterpillar(S)


## S2 <- S %>% spread(Parameter, value) %>%
##     apply(1, function(r) foo(ng, r[3], r[4]) %>% tbl_df %>% mutate(n=1:ng, ic=paste0(r[1], r[2]))) %>% bind_rows

fit <- data_frame(
    x=1:10,
    ymed=foo(10, median(S %>% filter(Parameter=="rho") %>% pull(value)),
        median(S %>% filter(Parameter=="theta") %>% pull(value))),
    ymin=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.025),
        median(S %>% filter(Parameter=="theta") %>% pull(value))),
    ymax=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.975),
        median(S %>% filter(Parameter=="theta") %>% pull(value)))
)



bind_rows(.id="set", ref=fc0, me=fc1) %>%
    ggplot() +
    geom_col(aes(x=n, y=value, fill=set), position="dodge") +
    geom_pointrange(data=fit, aes(x, ymed, ymin=ymin, ymax=ymax)) +
#    geom_line(data=S2, aes(x=n, y=value, group=ic), alpha=.02, color="grey30") +
    geom_line(data=fit, aes(x, y=ymin), color="blue") +
    geom_line(data=fit, aes(x, y=ymax), color="blue") +
    geom_line(data=fit, aes(x, y=ymed), color="red")


ggsave("img-freqspec-stan-fit-01.png")


## binom mic
img_bm <- stan_model("img-coalescent-binom-mix.stan")
#img_bm <- readRDS("img-coalescent-binom-mix.rds")

fc2 <- rbinoX(round(sum(fc1$value)), 1:10, .8, fc1$value, df=T)

m_img_bm <- sampling(
    img_bm,
    iter = 1000, chains=1,
    data=list(
        N=nrow(fc2), y=fc2$y, K=ng,
        sizes=1:ng, probs=rep(.8, ng)))

print(m_img_bm)
S <- ggs(m_img_bm)
ggs_traceplot(S) + facet_wrap(~Parameter, ncol=5)
ggs_caterpillar(S)

fd <- bind_rows(
    .id="set", ref=fc1,
    "p80"=fc2 %>% group_by(y) %>% summarize(n=n()) %>%
    select(value=n, n=y))


fc1 %>% mutate(prod=value*n) %>% summarize(total=sum(prod))
fc2 %>% group_by(y) %>% summarize(n=n()) %>% select(value=n, n=y) %>%
    mutate(prod=value*n) %>% summarize(total=sum(prod))

k <- replicate(1000,
rbinoX(round(sum(fc1$value)), 1:10, .8, fc1$value, df=T) %>%
    group_by(y) %>% summarize(n=n()) %>% select(value=n, n=y) %>%
    mutate(prod=value*n) %>% summarize(total=sum(prod)))
k <- k %>% unlist

summary(k)
ggplot(data_frame(x=k)) + aes(x) + geom_histogram() + geom_vline(xintercept=4000*.8) + geom_line(aes(x, y), data_frame(x=2800:3500, y=dpois(2800:3500, 4000 *.8) * 20000), color="red")


fit <- data_frame(
    x=1:10,
    ymed=foo(10, median(S %>% filter(Parameter=="rho") %>% pull(value)),
        #        median(S %>% filter(Parameter=="theta") %>% pull(value))),
        200),
    ymin=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.025),
        #        median(S %>% filter(Parameter=="theta") %>% pull(value))),
                200),
    ymax=foo(10, quantile(S %>% filter(Parameter=="rho") %>% pull(value),.975),
        #        median(S %>% filter(Parameter=="theta") %>% pull(value)))
                200)
)

fit

#    ymed=foo(10, .50, 199.95),
#    ymax=foo(10, .51, 202.95))


ggplot(data=fd) +
    geom_col(aes(x=n, y=value, fill=set), position=position_dodge(preserve = "single")) +
    geom_line(data=fit, aes(x, y=ymin), color="blue") +
    geom_line(data=fit, aes(x, y=ymax), color="blue") +
    geom_line(data=fit, aes(x, y=ymed), color="red")


library(magrittr)

params <- data_frame(
    rho=c(0.1, 0.25, 0.1, 0.25),
    theta = c(100, 100, 200, 200))

t1 <- apply(params, 1, function(r){ z <- foo(10,r[1], r[2]); z/sum(z) }) %>%
    tbl_df %>% set_colnames(paste(params$rho, params$theta)) %>% mutate( n=row_number()) %>%
    gather(key="rho_theta", value="value", -n)

ggplot(t1) + aes(x=n, y=value, color=rho_theta) + geom_line(aes(linetype=rho_theta))


################################################################################
## Priors
fc1
x <- 0:1000
d0 <- bind_rows(
    .id="dist",
    cauchy_50_5=data_frame(x=x, y=dcauchy(x,50,5)),
    norm_50_5=data_frame(x=x, y=dnorm(x,50,5)),
    gamma_s50_m50=data_frame(x=x, y=dgamma(x,5,scale=50)),
    cauchy_500_5=data_frame(x=x, y=dcauchy(x,500,5)),
    norm_500_5=data_frame(x=x, y=dnorm(x,500,5)),
    gamma_s500_m500=data_frame(x=x, y=dgamma(x,50,scale=500))
)

x <- 0:50
d0 <- bind_rows(
    .id="dist",
    gamma_s1_t2=data_frame(x=x, y=dgamma(x,1,scale=2)),
    gamma_s2_t2=data_frame(x=x, y=dgamma(x,2,scale=2)),
    gamma_s3_t2=data_frame(x=x, y=dgamma(x,3,scale=2)),
    gamma_s1_t4=data_frame(x=x, y=dgamma(x,1,scale=4)),
    gamma_s2_t4=data_frame(x=x, y=dgamma(x,2,scale=4)),
    gamma_s3_t4=data_frame(x=x, y=dgamma(x,3,scale=4))
)

x <- 0:5000
d0 <- bind_rows(
    .id="dist",
    cauchy_1000_5=data_frame(x=x, y=dcauchy(x,1000,1000)))

x <- 0:1500/10
d0 <- bind_rows(
    .id="dist",
    cauchy_05_5=data_frame(x=x, y=dcauchy(x,0.5,10)))


ggplot(d0) + aes(x,y,color=dist) + geom_line() + ylim(0,NA)
#+END_SRC

**** Estimating rho, theta and weights

This is somehow off. My rho estimates for incomplete data are always too low -
and I don't really understand what is going on..

#+BEGIN_SRC R
library(tidyverse)
theme_set(theme_bw())
library(rstan)
library(ggmcmc)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

img_fs_coalescent <- function(K, rho1, tau1, gess=0, df=FALSE) {
    spec <- (1:K)*0
    specprod1 <- (K+1 - 1:K)/(K+rho1 - 1:K)
    for (k in 1:K) spec[k] <- (tau1/k) * prod(specprod1[1:k])
    spec[K] <- spec[K] + gess
    if (df) data_frame(count=1:K, frequency=spec) else spec
}

#' @param n total number of observations
#' @param sizes number of trials for each component (single value will be recycled)
#' @param probs probabilities of success for each component (single value will be recycled)
#' @param weights of components, i.e. proportion of observation from total. Values will be normalized to sum to 1. Single values will be recycled.
#' 'size','prob' and 'weight'
#' @param df if TRUE, return data.frame including parameters used to simulate
#' each outcome.
rbinoX <- function(n, sizes, probs, weights=NULL, df=FALSE){
    if(is.null(weights)) weights <- 1
    cns <- sapply(list(sizes, probs, weights), length)
    cn <- max(cns)

    if(any(cns!=cn & cns!=1)) stop("sizes, probs and weights lengths differ. Need to either be the same or 1 (recycling)")

    if(length(sizes)==1) sizes <- rep(sizes, cn)
    if(length(probs)==1) probs <- rep(probs, cn)
    if(length(weights)==1) weights <- rep(weights, cn)

    r <- t(sapply(sample(1:cn, n, replace=TRUE, prob=weights), function(ci){
        c(sizes[ci], probs[ci], weights[ci], rbinom(1, sizes[ci], probs[ci]))}))
    if(df){
        r <- as.data.frame(r)
        colnames(r) <- c("size", "prob","weight","y")
        r
    }else{r[,4]}
}

f2c <- function(y, df=FALSE){
    x <- unlist(apply(y, 1, function(r) rep(r[1], r[2])))
    names(x) <- c()
    if (df) data_frame(gene=1:length(x), count=x) else x
}

c2f <- function(x, df=FALSE){
    y <- if (is.null(dim(x))) table(x) else table(x[,2])
    if (df) data_frame(count=names(y), frequency=as.vector(y)) else y
}

# N : number of genes (== gene families)
# K : number of genomes
# T : total gene counts

# x : gene counts (1..N), T = sum(x)
# y : gene count frequencies (1..K)

# rho (rho1) : rate of loosing a single gene with time units 2N_e
# tau (tau1) : average number of genes gained in 2N_e

K <- 10;
rho <- .5;
tau <- 200;
y <- img_fs_coalescent(K, rho, tau, df=TRUE)
T <- sum(apply(y, 1, prod))
x <- f2c(y, T)
N <- nrow(x)
c <- .8

data_frame(fs=y$frequency/sum(y$frequency)) %>% bind_cols(
               rstan::summary(m_img_bm, pars=c("phi"))$summary %>% tbl_df %>% dplyr::select(phi=mean),
               rstan::summary(m_img_bm, pars=c("lambda"))$summary %>% tbl_df %>% dplyr::select(lambda=mean))

 x80 <- rbinoX(N, 1:K, .8, y$frequency) %>% tbl_df %>%
    filter(value != 0) %>%
    arrange(value) %>%
    mutate(gene=row_number()) %>%
    dplyr::select(gene, count=value)

x50 <- rbinoX(N, 1:K, .5, y$frequency) %>% tbl_df %>%
    filter(value != 0) %>%
    arrange(value) %>%
    mutate(gene=row_number()) %>%
    dplyr::select(gene, count=value)

## binom mic
img_bm02
img_bm <- stan_model("img-coalescent-binom-mix-03.stan")
img_bm03 <- img_bm
#img_bm <- readRDS("img-coalescent-binom-mix.rds")

d100 <- list( N=nrow(x), K=K, x=x$count, c=rep(.9999, K))
d100
m_img_bm <- sampling(img_bm, iter = 500, chains=3, data=d100)

d080 <- list( N=nrow(x80), K=K, x=x80$count, c=rep(.80, K))
m_img_bm <- sampling(img_bm, iter = 500, chains=3, data=d080)

d050 <- list( N=nrow(x50), K=K, x=x50$count, c=rep(.50, K))
m_img_bm <- sampling(img_bm, iter = 500, chains=3, data=d050)

print(m_img_bm)
S <- ggs(m_img_bm)
ggs_traceplot(S %>% filter(Parameter %in% c("rho", "tau", "T"))) + facet_wrap(~Parameter, ncol=3, scales="free_y", strip.position="left")
ggs_caterpillar(S)

fit <- data_frame(
    x=1:K,
    ymed=img_fs_coalescent(K,
        median(S %>% filter(Parameter=="rho") %>% pull(value)),
        median(S %>% filter(Parameter=="tau") %>% pull(value))),
    ymin=img_fs_coalescent(K,
        quantile(S %>% filter(Parameter=="rho") %>% pull(value),.025),
        quantile(S %>% filter(Parameter=="tau") %>% pull(value),.025)),
    ymax=img_fs_coalescent(K,
        quantile(S %>% filter(Parameter=="rho") %>% pull(value),.975),
        quantile(S %>% filter(Parameter=="tau") %>% pull(value),.975)))

ggplot(data=y) +
    geom_col(aes(x=count, y=frequency), position=position_dodge(preserve = "single")) +
    geom_line(data=fit, aes(x, y=ymin), color="blue") +
    geom_line(data=fit, aes(x, y=ymax), color="blue") +
    geom_line(data=fit, aes(x, y=ymed), color="red")
#+END_SRC

**** REJC Construct Dirichlet prior from IMG freq spectrum
     CLOSED: [2017-11-07 Tue 15:16]

#+BEGIN_SRC R
library(tidyverse)
library(magrittr)
library(MCMCpack)

img_fs_coalescent <- function(K, rho1, tau1, gess=0, df=FALSE) {
    spec <- (1:K)*0
    specprod1 <- (K+1 - 1:K)/(K+rho1 - 1:K)
    for (k in 1:K) spec[k] <- (tau1/k) * prod(specprod1[1:k])
    spec[K] <- spec[K] + gess
    if (df) data_frame(count=1:K, frequency=spec) else spec
}

K <- 10
rho <- .5
tau <- 200
fs <- img_fs_coalescent(K, rho, tau)

kappa <- 1000         # weights
phi <- fs/sum(fs)     # expected relative mean freqs
alpha = kappa * phi

rfs <- data_frame(prob=fs/sum(fs), cat=sprintf("%03d",1:K))
dr <- rdirichlet(1000, alpha) %>% tbl_df %>%
    set_colnames(sprintf("%03d",1:K)) %>%
    gather("cat", "prob")
ggplot(dr) + aes(y=prob, x=cat, fill=cat) + geom_boxplot() +
    geom_point(data=rfs)


lambdas <- matrix(c(
    0.0419876,0.0180973,0.154338,0.403514,0.20848,0.0491288,0.0292669,0.0121678,0.017921,0.0650986,
    0.0148997,0.105448,0.0268778,0.223929,0.108628,0.05415,0.0620618,0.0419299,0.295009,0.0670672,
    0.266975,0.168605,0.0126078,0.283949,0.0608623,0.0597973,0.0184083,0.0494776,0.0382478,0.0410704,
    0.0287053,0.0320149,0.250389,0.246487,0.0187187,0.0324918,0.108055,0.0901247,0.123494,0.0695204,
    0.0729181,0.248926,0.0781635,0.103866,0.213705,0.00951561,0.0121616,0.197069,0.0203873,0.043288,
    0.122415,0.334449,0.241238,0.0751331,0.0950512,0.0114062,0.0266597,0.0639929,0.0237863,0.00586927), ncol=6) %>% t() %>% tbl_df %>%
    set_colnames(sprintf("%03d",1:K)) %>%
    gather("cat", "prob")

ggplot(lambdas) + aes(y=prob, x=cat, fill=cat) + geom_boxplot()
#+END_SRC

**** REJC write a binomial_mixture_lpmf
     CLOSED: [2017-11-07 Tue 11:55]
file:binom-mix-lpmf.stan

#+BEGIN_SRC R
library(tidyverse)
library(rstan)
# need to understand stan matrices (or 2-dimensional arrays)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

sm <- stan_model("binom-mix-lpmf.stan")

fit <- sampling(sm, iter=1, chain=1,  algorithm="Fixed_param")

sc <-'
functions {
  // IMG coalescent frequency spectrum
  vector fs_coalescent(int K, real rho, real tau){
    vector[K] fs = rep_vector(0, K);    // freq spectrum
    vector[K] fsp;                      // freq product
    for(k in 1:K)                       // cache prod series
      fsp[k] = (K+1 - k)/(K+rho - k);
    for(k in 1:K)                       // summarize series with length 1,2,...,k
      fs[k] = tau/k * prod(segment(fsp, 1, k));
    return fs;
  }
}
model{
  print(fs_coalescent(10, .5, 200))
}
'

sm <- stan_model(model_code=sc)
fit <- sampling(sm, iter=1, chain=1,  algorithm="Fixed_param")
#+END_SRC

**** REJC model IMG frequency spectrum with binomial in estimate
     CLOSED: [2017-11-07 Tue 15:20]
Go the other way - start with IMG estimate, model with incomplete, fit to data...

#+BEGIN_SRC R
library(tidyverse)
theme_set(theme_bw())
library(rstan)
library(ggmcmc)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

img_fs_coalescent <- function(K, rho1, tau1, gess=0, df=FALSE) {
    spec <- (1:K)*0
    specprod1 <- (K+1 - 1:K)/(K+rho1 - 1:K)
    for (k in 1:K) spec[k] <- (tau1/k) * prod(specprod1[1:k])
    spec[K] <- spec[K] + gess
    if (df) data_frame(count=1:K, frequency=spec) else spec
}


f2c <- function(y, df=FALSE){
    x <- unlist(apply(y, 1, function(r) rep(r[1], r[2])))
    names(x) <- c()
    if (df) data_frame(gene=1:length(x), count=x) else x
}

c2f <- function(x, df=FALSE){
    y <- if (is.null(dim(x))) table(x) else table(x[,2])
    if (df) data_frame(count=names(y), frequency=as.vector(y)) else y
}

# N : number of genes (== gene families)
# K : number of genomes
# T : total gene counts

# x : gene counts (1..N), T = sum(x)
# y : gene count frequencies (1..K)

# rho (rho1) : rate of loosing a single gene with time units 2N_e
# tau (tau1) : average number of genes gained in 2N_e

K <- 10;
rho <- .5;
tau <- 200;
y <- img_fs_coalescent(K, rho, tau, df=TRUE) %>% round
T <- sum(apply(y, 1, prod))
x <- f2c(y, T)
N <- nrow(x)
c <- .8



y80 <- y %>% round %>% apply(1, function(r){
    k <- r[1]
    freq <- r[2]
    rbinom(freq, k, c) %>% table %>% tbl_df %>%
        select(count=1, frequency=2)}) %>%
        reduce(full_join, by = "count") %>%
        mutate(frequency=rowSums(select(., -count), na.rm=T)) %>%
        select(count, frequency)

x80 <- f2c(y80, T)

img_bm <- stan_model("img-coalescent-binom-mix-04.stan")

d100 <- list(K=K, N=N, x=x$count, c=.999)
m_img_bm <- sampling(img_bm, iter = 1, chains=1, data=d100)

d080 <- list( K=K, N=nrow(x80), x=x80$count, c=.8)
m_img_bm <- sampling(img_bm, iter = 500, chains=3, data=d080)

d050 <- list( N=nrow(x50), K=K, x=x50$count, c=rep(.50, K))
m_img_bm <- sampling(img_bm, iter = 500, chains=3, data=d050)

print(m_img_bm)
S <- ggs(m_img_bm)
ggs_traceplot(S %>% filter(Parameter %in% c("rho", "tau", "T"))) + facet_wrap(~Parameter, ncol=3, scales="free_y", strip.position="left")
ggs_caterpillar(S)

fit <- data_frame(
    x=1:K,
    ymed=img_fs_coalescent(K,
        median(S %>% filter(Parameter=="rho") %>% pull(value)),
        median(S %>% filter(Parameter=="tau") %>% pull(value))),
    ymin=img_fs_coalescent(K,
        quantile(S %>% filter(Parameter=="rho") %>% pull(value),.025),
        quantile(S %>% filter(Parameter=="tau") %>% pull(value),.025)),
    ymax=img_fs_coalescent(K,
        quantile(S %>% filter(Parameter=="rho") %>% pull(value),.975),
        quantile(S %>% filter(Parameter=="tau") %>% pull(value),.975)))

ggplot(data=y) +
    geom_col(aes(x=count, y=frequency), position=position_dodge(preserve = "single")) +
    geom_line(data=fit, aes(x, y=ymin), color="blue") +
    geom_line(data=fit, aes(x, y=ymax), color="blue") +
    geom_line(data=fit, aes(x, y=ymed), color="red")
#+END_SRC

**** REJC another look at a simple binom mix
     CLOSED: [2017-11-07 Tue 15:22]

#+BEGIN_SRC R
library(tidyverse)
theme_set(theme_bw())
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())


scode <- '
data {
  int K;
  int N;
  int x[N];
  real p;
}
parameters {
  simplex[K] theta; // component weights
}
model {
  real lp;
  vector[K] log_theta;
  log_theta = log(theta);
  for (n in 1:N){
    // print("");
    for (k in x[n]:K){
      // print(x[n], ":", k);
      lp = binomial_lpmf(x[n] | k, p);
      // print("lp: ", lp);
      target += log_sum_exp(lp, log_theta[k]);
    }
  }
}
'

scomp <- stan_model(model_code=scode)

p <- .8
x <- rbinom(100, 80, .8)
x2 <- c(rbinom(500, 79, .8), rbinom(500, 81, .8))

ggplot(data_frame(x=c(x, x2), g=rep("80", "79,81", each=1000))) + geom_bar(aes(x))

data=list(K=100, N=length(x), x=x, p=p)
sfit <- sampling(scomp, iter=500, chain=4, data=data)

stan_plot(sfit)
stan_trace(sfit)
stan_scat(sfit)
stan_hist(sfit)
stan_dens(sfit)
stan_ac(sfit)
#+END_SRC

*** /Regroup/ - IMG in Stan II
**** Directly compute expected IMG spectrum given binomial process (R and Stan code)

#+BEGIN_SRC R
library(tidyverse)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())


img <- function(K, rho, theta) {
  spec <- (1:K)*0
  (specprod1 <- (K+1 - 1:K)/(K+rho - 1:K))
  for (k in 1:K) {
      spec[k] <- prod(specprod1[1:k]) * (theta/k)
  }
  return(spec)
}


bmiss <- function(x, p, with_zero=FALSE){
    r <- x/sum(x)
    K <- length(x)
    dmix <- rep(0, K+1)
    for(k in 1:K){
        dmix[1:(k+1)] = dmix[1:(k+1)] + dbinom(0:k, k, p) * r[k]
    }
    if(with_zero) dmix * sum(x) else (dmix * sum(x))[-1]
}

## de-vectorized: better stan template
bmiss2 <- function(x, p, with_zero=FALSE){
    r <- x/sum(x)
    K <- length(x)
    dmix <- rep(0, K+1)
    print(log(dmix))
    for(k in 1:K){
        for(j in 0:k){
            dmix[j+1] = dmix[j+1] + dbinom(j, k, p) * r[k]
        }
    }
    print(log(dmix))
    if(with_zero) dmix * sum(x) else (dmix * sum(x))[-1]
}

bmiss_log <- function(x, p, with_zero=FALSE){
    r <- log(x/sum(x))
    K <- length(x)
    lp <- rep(-Inf, K+1)
    print(lp)
    for(k in 1:K){
        for(j in 0:k){
            lp[j+1] = log(exp(lp[j+1]) + exp(dbinom(j, k, p, log=T) + r[k]))
        }
    }
    print(exp(lp))
    if(with_zero) exp(lp) * sum(x) else (exp(lp) * sum(x))[-1]
}



fs <- img(10, .5, 200); round(fs);
fsm <- bmiss(fs, .8); round(fsm)
fsm2 <- bmiss2(fs, .8); round(fsm2)
fsml <- bmiss_log(fs, .8); round(fsml)


## bmiss in Stan
scomp <- stan_model("img-bmiss.stan")

data=list(K=10, rho=.5, theta=2000, p=0.8)
sfit <- sampling(scomp, iter=1, chain=1, data=data, algorithm="Fixed_param")

sfit

stan_plot(sfit)
stan_trace(sfit)
stan_scat(sfit)
stan_hist(sfit)
stan_dens(sfit)
stan_ac(sfit)



## potential extension: Poisson-Binomial (with sampling of unobserved p) to
## account for variance in p
library(poibin)

## dmix[1:(k+1)] = dmix[1:(k+1)] + dpoibin(0:k, p) * r[k]

d0 <- data_frame(x=c(
                     rbinom(1000, 100, .6),
                     rpoibin(1000, rep(c(.3,.9), each=50))),
                 g=rep(c(".6", ".3/.9"), each=1000))

ggplot(d0) + aes(x=x, fill=g) + geom_bar(position=position_dodge(preserve="single"))
#+END_SRC

**** Fit incomplete IMG estimate using Stan

#+BEGIN_SRC R
library(tidyverse)
theme_set(theme_bw())
library(rstan)
library(ggmcmc)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(magrittr)

img_fs <- function(K, rho1, tau1, gess=0, df=FALSE) {
    spec <- (1:K)*0
    specprod1 <- (K+1 - 1:K)/(K+rho1 - 1:K)
    for (k in 1:K) spec[k] <- (tau1/k) * prod(specprod1[1:k])
    spec[K] <- spec[K] + gess
    if (df) data_frame(count=1:K, frequency=spec) else spec
}


f2c <- function(y, df=FALSE){
    x <- unlist(apply(y, 1, function(r) rep(r[1], r[2])))
    names(x) <- c()
    if (df) data_frame(gene=1:length(x), count=x) else x
}

c2f <- function(x, df=FALSE){
    y <- if (is.null(dim(x))) table(x) else table(x[,2])
    if (df) data_frame(count=names(y), frequency=as.vector(y)) else y
}

bmiss <- function(x, p, with_zero=FALSE){
    r <- x/sum(x)
    K <- length(x)
    dmix <- rep(0, K+1)
    for(k in 1:K){
        dmix[1:(k+1)] = dmix[1:(k+1)] + dbinom(0:k, k, p) * r[k]
    }
    if(with_zero) dmix * sum(x) else (dmix * sum(x))[-1]
}

hide <- function(y, c, with_zero=FALSE){
    K <- max(y[,1]);
    z <- data_frame(Var1=as.character(0:K), n=0)

    r <- apply(y, 1, function(r) rbinom(r[2], r[1], c)) %>%
        map(table) %>% map(tbl_df) %>%
        bind_rows(z) %>% select(count=Var1, frequency=n) %>%
        group_by(count) %>% summarize(frequency=sum(frequency)) %>%
        mutate(count=as.numeric(count)) %>% arrange(count)
    if(with_zero) r else r %>% filter(count!=0)
}

# N : number of genes (== gene families)
# K : number of genomes
# T : total gene counts

# x : gene counts (1..N), T = sum(x)
# y : gene count frequencies (1..K)

# rho (rho1) : rate of loosing a single gene with time units 2N_e
# tau (tau1) : average number of genes gained in 2N_e

K <- 10;
rho <- .5;
tau <- 500;
y <- img_fs_coalescent(K, rho, tau, df=TRUE)
T <- sum(apply(y, 1, prod))
x <- f2c(y, T)
N <- nrow(x)
c <- .8

rhos <- c(c(.5,1:10,15)/5)
cs <- c(1, .8, .5)
params <- data_frame(
    rho=rep(rhos, length(cs)),
    c=rep(cs, each=length(rhos))
) %>%
    merge(data_frame(K=c(10,20,100,500))) %>%
    unite("params", everything(), remove=FALSE)

## hide and bmiss generate conistent data
img_fs(10, .5, 200, df=T) %>% hide(c) %>% select(2)
img_fs(10, .5, 200) %>% bmiss(c)

img_fs(10, .4, .3) %T>% print  %>% bmiss(c)


set.seed(1337)
f0 <- params %>% split(.$params) %>%
    map_dfr(function(p){
        merge(p, img_fs(p$K, p$rho, tau, df=TRUE)
              %>% hide(p$c))}) %>% tbl_df

m0 <- params %>% split(.$params) %>%
    map_dfr(function(p){
        merge(p, img_fs(p$K, p$rho, tau) %>%
                  bmiss(p$c) %>%
                  data_frame(count=1:length(.), frequency=.))}) %>% tbl_df

f0
m0

d0 <- bind_rows(
    .id="set",
    sim=f0,
    exp=m0)


ggplot(d0) +
    geom_line(data=m0, aes(x=count, y=frequency, color=as.factor(rho))) +
    geom_point(data=f0, aes(x=count, y=frequency, color=as.factor(rho))) +
    #scale_y_log10(limits=c(1e-2, NA)) +
    #geom_hline(yintercept=tau) +
    facet_grid(c~K, scales="free_x")

ggsave("rho.png")

f0 <- params %>% split(.$params) %>%
    map_dfr(function(p){
        merge(p, img_fs_coalescent(p$K, p$rho, tau, df=TRUE)
              %>% hide(p$c))}) %>% tbl_df


## STAN

d0 <- params %>% split(.$params) %>%
    map_dfr(function(p){
        merge(p, img_fs_coalescent(p$K, p$rho, tau, df=TRUE)
              %>% hide(p$c) %>% f2c(df=T))}) %>% tbl_df

K <- 10
rho <- .5
theta <- 200
p <- .8

(y <- img_fs(K, rho, theta))


theta <- .5 * 4000 / 10
rnorm(1000, theta, sqrt(theta*10)) %>% hist

smod <- stan_model("img-bmiss-02.stan")

p <- 1
y <- img_fs(K, rho, theta)
data <- list(K=K, M=as.integer(sum(y*1:length(y))), y=y, p=p)

p <- .8
y <- img_fs(K, rho, theta, df=T) %>% hide(p) %>% pull(frequency)
data <- list(K=K, M=as.integer(sum(y*1:length(y))), y=y, p=p)
sfit <- sampling(smod, iter=500, chain=4, data=data)

p <- .5
y <- img_fs(K, rho, theta, df=T) %>% hide(p) %>% pull(frequency)
data <- list(K=K, M=as.integer(sum(y*1:length(y))), y=y, p=p)
sfit <- sampling(smod, iter=500, chain=4, data=data)


stan_plot(sfit)
stan_trace(sfit)
stan_scat(sfit, pars=c("rho", "theta"))
stan_hist(sfit)
stan_dens(sfit)
stan_ac(sfit)


f0

sargs <- list(iter=1, chains=1)

run_stan <- function(d){
    sargs_data <- sargs
    sargs_data$data <- list(N=nrow(d), K=d$K[1], x=d$count, c=d$c)
    print(sargs_data)
    sampling(smod, sargs)
}

d0

d1 <- d0 %>% filter(count!=0) %>% filter(K==10, c==1, rho==2)
dtt=list(N=nrow(d1), K=d1$K[1], x=d1$count, c=rep(d1$c[1]-0.001, d1$K[1]))
dtt
#+END_SRC

**** DISC Gene counts on incomplete data are Bernulli trails with different p -> Poisson Binomial
See hard-copy notes 2017/11/02

Each gene cluster is a sequence of Bernulli trials (single event Binomial), with
potentially different p for each trial. The outcome for the entire gene is the
sum (or convolution) of the Bernulli trials.

For a single p (p_1=p_2=..=p_n), this is simply a Binomial distribution.

For different p, it turns into a Poisson-Binomial distribution - with an ugly
PMF/CDF, try to avoid for now. Though, it comes with easy estimators for moments
(mean + variance)

#+BEGIN_SRC R
library(tidyverse)

rbinom_probmix <- function(n, size, probs){
    replicate(N, sum(sapply(sample(probs, size), function(p) rbinom(1, 1, p))))
}



dbinom_probmix <- function(x, size, probs){
    # this actually doesn't work - this should be something like a
    # Poisson-Binomial distribution, i.e. a sequence of Bernulli's with
    # different probs. However, the pmf/CDF of Poisson-Binomial are quite
    # ugly...
}

d1 <- bind_rows(
    .id="set",
    p100=data_frame(count=1:Kmax, freq=dbinom(1:Kmax, K, 1)),
    p080=data_frame(count=1:Kmax, freq=dbinom(1:Kmax, K, p)))


N <- 1000
K <- 100
p <- .6
p_mix <- rep(c(.2, 1), each=K/2)
Kmax = K*1.05

r1 <- bind_rows(
    .id="set",
    #p100=data_frame(count=K, freq=N),
    p080=rbinom(N, K, p) %>% table %>% tbl_df %>%
        select(count=1, freq=2) %>%
        mutate(count=as.numeric(count)),
    `p060-p100`=rbinom_probmix(N,K,p_mix) %>% table %>% tbl_df %>%
        select(count=1, freq=2) %>%
        mutate(count=as.numeric(count)),
    p080n=rnorm(N, K*p, log(K)) %>% round %>% table %>% tbl_df %>%
        select(count=1, freq=2) %>%
        mutate(count=as.numeric(count))
)
str(r1)

ggplot(r1) + aes(x=count, y=freq, fill=set) + xlim(0,NA) +
    #geom_col(position=position_dodge(preserve="single")) +
    geom_density(stat="identity", alpha=.3)
#+END_SRC



*** More TODOs
**** Estimating rho1/2, theta1/2, weights and core
**** Fitting real data
